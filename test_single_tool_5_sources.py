#!/usr/bin/env python3
"""
Focused test to generate a single tool with all 5 sources and display the AI response.
This will show you the actual content generated by our cleaned prompt system.
"""

import asyncio
import sys
import os
from pathlib import Path
import json
import time

# Add the dashboard_app to path
sys.path.insert(0, '/Users/Dimar/Documents/python-code/MTSA/tools-dashboard/dashboard_app')

from database_implementation.precomputed_findings_db import get_precomputed_db_manager
from key_findings.unified_ai_service import UnifiedAIService
from key_findings.prompt_engineer import PromptEngineer

class SingleTool5SourcesTest:
    """Test class for single tool with all 5 sources."""

    def __init__(self):
        self.db_manager = get_precomputed_db_manager()
        self.ai_service = UnifiedAIService()
        self.prompt_engineer = PromptEngineer(language="es")

        # Test with a specific tool
        self.test_tool = "Benchmarking"
        self.test_sources = ["Google Trends", "Google Books", "Bain Usability", "Bain Satisfaction", "Crossref"]
        self.test_language = "es"

    async def generate_analysis(self):
        """Generate analysis for single tool with 5 sources."""

        print("üß™ Single Tool + 5 Sources Test")
        print("=" * 60)
        print(f"Tool: {self.test_tool}")
        print(f"Sources: {', '.join(self.test_sources)}")
        print(f"Language: {self.test_language}")
        print("=" * 60)

        # Prepare test data
        test_data = {
            "tool_name": self.test_tool,
            "selected_sources": self.test_sources,
            "source_name": None,  # Multi-source
            "language": self.test_language,
            "date_range_start": "1950-01-01",
            "date_range_end": "2023-12-31",
            "data_points_analyzed": 888,
            "pca_insights": {
                "dominant_patterns": [
                    {"component": 1, "loadings": {"Google Trends": 0.8, "Google Books": 0.7, "Bain Usability": 0.6, "Bain Satisfaction": 0.5, "Crossref": 0.9}}
                ],
                "total_variance_explained": 78.5
            },
            "heatmap_analysis": {
                "correlation_matrix": "sample_correlation_data",
                "strong_correlations": 3,
                "weak_correlations": 2
            },
            "analysis_type": "multi_source"
        }

        # Generate the prompt using our cleaned prompt system
        print("\n1. üìù Generating prompt with cleaned system...")
        prompt = self.prompt_engineer.create_analysis_prompt(test_data, {})

        print(f"\nüìÑ PROMPT LENGTH: {len(prompt)} characters")
        print(f"üìù PROMPT PREVIEW (first 500 chars):")
        print("-" * 50)
        print(prompt[:500] + "..." if len(prompt) > 500 else prompt)
        print("-" * 50)

        # Call AI service
        print("\n2. ü§ñ Calling AI service...")
        start_time = time.time()

        try:
            response = await self.ai_service.generate_analysis(
                prompt=prompt,
                language=self.test_language
            )

            processing_time = time.time() - start_time

            print(f"\n‚è±Ô∏è  PROCESSING TIME: {processing_time:.2f} seconds")
            print(f"üìä RESPONSE TYPE: {type(response)}")
            print(f"üìè RESPONSE LENGTH: {len(str(response))} characters")

            # Display the complete response
            print(f"\n3. üìã COMPLETE AI RESPONSE:")
            print("=" * 80)

            if isinstance(response, dict):
                print("üîç STRUCTURED RESPONSE (Parsed JSON):")
                print(json.dumps(response, indent=2, ensure_ascii=False))

                # Extract key sections
                print(f"\n4. üîë KEY SECTIONS EXTRACTED:")
                print("-" * 40)

                if "executive_summary" in response:
                    print(f"üìÑ Executive Summary:")
                    print(response["executive_summary"][:300] + "..." if len(response["executive_summary"]) > 300 else response["executive_summary"])
                    print()

                if "principal_findings" in response:
                    print(f"üìä Principal Findings:")
                    findings = response["principal_findings"]
                    if isinstance(findings, list):
                        for i, finding in enumerate(findings[:3], 1):  # Show first 3
                            if isinstance(finding, dict) and "bullet_point" in finding:
                                print(f"  {i}. {finding['bullet_point']}")
                            elif isinstance(finding, str):
                                print(f"  {i}. {finding}")
                    print()

                if "pca_analysis" in response:
                    print(f"üìà PCA Analysis:")
                    print(response["pca_analysis"][:200] + "..." if len(response["pca_analysis"]) > 200 else response["pca_analysis"])
                    print()

                # Count total findings
                total_findings = len(response.get("principal_findings", []))
                print(f"üìä TOTAL FINDINGS: {total_findings}")

            else:
                print("üìù RAW RESPONSE:")
                print(str(response))

            print("=" * 80)

            # Store in database for verification
            print(f"\n5. üíæ Storing in database...")

            analysis_data = {
                "executive_summary": response.get("executive_summary", "") if isinstance(response, dict) else str(response),
                "principal_findings": response.get("principal_findings", "") if isinstance(response, dict) else str(response),
                "pca_analysis": response.get("pca_analysis", "") if isinstance(response, dict) else "",
                "temporal_analysis": response.get("temporal_analysis", "") if isinstance(response, dict) else "",
                "seasonal_analysis": response.get("seasonal_analysis", "") if isinstance(response, dict) else "",
                "fourier_analysis": response.get("fourier_analysis", "") if isinstance(response, dict) else "",
                "heatmap_analysis": response.get("heatmap_analysis", "") if isinstance(response, dict) else "",
                "tool_display_name": self.test_tool,
                "data_points_analyzed": response.get("data_points_analyzed", 888) if isinstance(response, dict) else 888,
                "confidence_score": response.get("confidence_score", 0.92) if isinstance(response, dict) else 0.92,
                "model_used": response.get("model_used", "moonshotai/kimi-k2-instruct") if isinstance(response, dict) else "moonshotai/kimi-k2-instruct",
                "analysis_type": "multi_source",
                "processing_time": processing_time,
                "timestamp": time.time()
            }

            # Generate combination hash
            from database_implementation.precomputed_findings_db import generate_combination_hash
            combination_hash = generate_combination_hash(
                tool_name=self.test_tool,
                sources=self.test_sources,
                language=self.test_language
            )

            # Store in database
            record_id = self.db_manager.store_precomputed_analysis(
                combination_hash=combination_hash,
                tool_name=self.test_tool,
                selected_sources=self.test_sources,
                language=self.test_language,
                analysis_data=analysis_data
            )

            print(f"‚úÖ Record stored with ID: {record_id}")
            print(f"üîó Combination hash: {combination_hash}")

            return response, record_id

        except Exception as e:
            print(f"‚ùå Error during AI generation: {e}")
            import traceback
            traceback.print_exc()
            return None, None

def main():
    """Main function to run the test."""

    print("üöÄ Starting Single Tool + 5 Sources Test")
    print("This will show you the actual AI response using our cleaned prompts!")
    print()

    # Check API key
    if not os.environ.get("GROQ_API_KEY"):
        print("‚ö†Ô∏è  Warning: GROQ_API_KEY not found in environment")
        print("   Make sure you have: export GROQ_API_KEY='your_key'")
        print()

    # Run the test
    test = SingleTool5SourcesTest()

    async def run_async():
        response, record_id = await test.generate_analysis()

        if response and record_id:
            print(f"\nüéâ Test completed successfully!")
            print(f"   Record ID: {record_id}")
            print(f"   Response type: {type(response)}")
            print(f"   Content quality: {'‚úÖ High' if isinstance(response, dict) and len(str(response)) > 1000 else '‚ö†Ô∏è  Medium'}")
        else:
            print("\n‚ùå Test failed")
            return 1

        return 0

    # Run async function
    return asyncio.run(run_async())

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)