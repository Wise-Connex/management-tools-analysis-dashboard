#!/usr/bin/env python3
"""
AI Integration Test Script
Tests integration between existing AI analysis logic and new precomputed findings database.
"""

import sys
import os
import json
import time
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "dashboard_app"))

from database_implementation.precomputed_findings_db import get_precomputed_db_manager


def test_ai_integration_single_source():
    """Test AI integration for single source analysis."""
    print("\nüß™ Testing AI Integration - Single Source Analysis")
    print("=" * 50)

    try:
        # Initialize services (simplified for testing)
        precomputed_db = get_precomputed_db_manager()

        # For testing, we'll simulate the AI analysis since we may not have API keys
        print("üìã Simulating single source analysis...")

        # Test combination
        tool_name = "Benchmarking"
        selected_sources = ["Google Trends"]  # Single source
        language = "es"

        # Generate hash for this combination
        combination_hash = precomputed_db.generate_combination_hash(
            tool_name, selected_sources, language
        )
        print(f"üîë Generated hash: {combination_hash}")

        # Simulate AI analysis results (in real implementation, this would call the AI service)
        simulated_ai_result = {
            "executive_summary": f"""# An√°lisis Ejecutivo - {tool_name}

## Resumen General
Este an√°lisis examina las tendencias de b√∫squeda para "{tool_name}" utilizando datos de Google Trends. 

## Hallazgos Principales
- **Tendencia General**: Crecimiento sostenido en el inter√©s de b√∫squeda
- **Patr√≥n Estacional**: Picos identificados durante ciertos per√≠odos del a√±o
- **Volatilidad**: Nivel moderado de variaci√≥n en las b√∫squedas

## Implicaciones Estrat√©gicas
Los resultados sugieren una creciente adopci√≥n de herramientas de {tool_name} en el mercado espa√±ol.""",
            "temporal_analysis": f"""# An√°lisis Temporal - {tool_name}

## Tendencias Observadas
El an√°lisis temporal revela un **crecimiento positivo** en las b√∫squedas relacionadas con {tool_name}. La tendencia muestra:

- **Tendencia Lineal**: Pendiente positiva de 0.15 puntos por mes
- **Volatilidad**: Desviaci√≥n est√°ndar de 2.3 puntos
- **Momento**: Aceleraci√≥n del 12% en los √∫ltimos 6 meses

## Volatilidad y Estabilidad
Los datos muestran un nivel de volatilidad **moderado** con per√≠odos de estabilidad seguidos de aumentos significativos.""",
            "seasonal_analysis": f"""# An√°lisis Estacional - {tool_name}

## Patrones Estacionales Identificados
Se detectaron **patrones estacionales claros** en las b√∫squedas de {tool_name}:

- **Temporada Alta**: Marzo-Mayo y Septiembre-Noviembre
- **Temporada Baja**: Junio-Agosto y Diciembre-Febrero
- **Fuerza Estacional**: 0.68 (fuerte estacionalidad)

## Periodicidad
La periodicidad principal es **anual** con algunos componentes semestrales menores.""",
            "fourier_analysis": f"""# An√°lisis de Fourier - {tool_name}

## Frecuencias Dominantes
El an√°lisis espectral identifica las siguientes frecuencias principales:

1. **Frecuencia Principal**: 0.083 cycles/month (per√≠odo: 12 meses)
   - **Potencia**: 85% de la se√±al total
   - **Significancia**: Altamente significativa

2. **Frecuencia Secundaria**: 0.167 cycles/month (per√≠odo: 6 meses)
   - **Potencia**: 12% de la se√±al total
   - **Significancia**: Moderadamente significativa

## Calidad de la Se√±al
- **Relaci√≥n Se√±al/Ruido**: 8.2 (excelente)
- **Potencia Total**: 1,247 unidades
- **Potencia de Ruido**: 152 unidades""",
            "heatmap_analysis": f"""# An√°lisis de Calor - {tool_name}

## Distribuci√≥n de Datos
El an√°lisis de densidad revela patrones claros en los datos de {tool_name}:

- **Regiones de Alta Densidad**: Concentraci√≥n en valores medios-altos
- **Regiones de Baja Densidad**: Valores extremos (muy bajos y muy altos)
- **Gradientes**: Transiciones suaves entre regiones de densidad

## Clusters Identificados
Se detectaron **2 clusters principales**:
1. Cluster Principal: 67% de los datos (tendencia creciente)
2. Cluster Secundario: 23% de los datos (variaci√≥n estacional)

## Outliers
- **Valores At√≠picos**: 3 puntos outlier detectados
- **Impacto**: M√≠nimo en el an√°lisis general""",
            "data_points_analyzed": 2847,
            "confidence_score": 0.89,
            "model_used": "gpt-4",
            "analysis_type": "single_source",
            "tool_display_name": tool_name,
        }

        # Store the analysis in database
        print("üíæ Storing analysis in database...")
        record_id = precomputed_db.store_precomputed_analysis(
            combination_hash=combination_hash,
            tool_name=tool_name,
            selected_sources=selected_sources,
            language=language,
            analysis_data=simulated_ai_result,
        )
        print(f"‚úÖ Stored analysis record ID: {record_id}")

        # Retrieve the analysis
        print("üîç Retrieving analysis from database...")
        retrieved_analysis = precomputed_db.get_combination_by_hash(combination_hash)

        if retrieved_analysis:
            print("‚úÖ Successfully retrieved analysis")
            print(f"   - Record ID: {retrieved_analysis['id']}")
            print(f"   - Tool: {retrieved_analysis['tool_name']}")
            print(f"   - Analysis Type: {retrieved_analysis['analysis_type']}")
            print(f"   - Confidence: {retrieved_analysis['confidence_score']}")
            print(
                f"   - Executive Summary Length: {len(retrieved_analysis['executive_summary'])}"
            )

            # Verify content preservation
            content_preserved = (
                retrieved_analysis["executive_summary"]
                == simulated_ai_result["executive_summary"]
                and retrieved_analysis["temporal_analysis"]
                == simulated_ai_result["temporal_analysis"]
                and retrieved_analysis["seasonal_analysis"]
                == simulated_ai_result["seasonal_analysis"]
                and retrieved_analysis["fourier_analysis"]
                == simulated_ai_result["fourier_analysis"]
                and retrieved_analysis["heatmap_analysis"]
                == simulated_ai_result["heatmap_analysis"]
            )

            print(
                f"   - Content Preservation: {'‚úÖ PASSED' if content_preserved else '‚ùå FAILED'}"
            )

            # Test markdown formatting
            markdown_preserved = (
                "#" in retrieved_analysis["executive_summary"]
                and "##" in retrieved_analysis["temporal_analysis"]
                and "**" in retrieved_analysis["seasonal_analysis"]
            )
            print(
                f"   - Markdown Formatting: {'‚úÖ PASSED' if markdown_preserved else '‚ùå FAILED'}"
            )

            return {
                "success": True,
                "record_id": record_id,
                "content_preserved": content_preserved,
                "markdown_preserved": markdown_preserved,
                "retrieved_data": retrieved_analysis,
            }
        else:
            print("‚ùå Failed to retrieve analysis")
            return {"success": False, "error": "Retrieval failed"}

    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        import traceback

        traceback.print_exc()
        return {"success": False, "error": str(e)}


def test_ai_integration_multi_source():
    """Test AI integration for multi-source analysis."""
    print("\nüß™ Testing AI Integration - Multi Source Analysis")
    print("=" * 50)

    try:
        # Initialize database manager
        precomputed_db = get_precomputed_db_manager()

        # Test combination (multiple sources)
        tool_name = "Calidad Total"
        selected_sources = [
            "Google Trends",
            "Google Books",
            "Bain Usability",
        ]  # Multiple sources
        language = "es"

        # Generate hash for this combination
        combination_hash = precomputed_db.generate_combination_hash(
            tool_name, selected_sources, language
        )
        print(f"üîë Generated hash: {combination_hash}")

        # Simulate multi-source AI analysis results
        simulated_multi_source_result = {
            "executive_summary": f"""# An√°lisis Ejecutivo - {tool_name}
## An√°lisis Multifuente Integrado

Este an√°lisis combina datos de **3 fuentes** para proporcionar una visi√≥n comprehensiva de {tool_name}:

### Fuentes Analizadas
- **Google Trends**: Tendencias de b√∫squeda y inter√©s p√∫blico
- **Google Books**: Publicaciones acad√©micas y literatura
- **Bain Usabilidad**: Datos de uso y adopci√≥n empresarial

### Hallazgos Principales Integrados
La convergencia de datos de m√∫ltiples fuentes revela patrones consistentes sobre {tool_name}:

1. **Consenso Intersource**: Las tres fuentes confirman el crecimiento
2. **Diferencias Temporales**: Bain muestra adopci√≥n m√°s temprana
3. **Validaci√≥n Cruzada**: Correlaci√≥n del 0.87 entre fuentes

### Implicaciones Estrat√©gicas
El an√°lisis multifuente valida las tendencias identificadas y proporciona mayor confianza en las proyecciones.""",
            "principal_findings": f"""# Hallazgos Principales - An√°lisis PCA

## An√°lisis de Componentes Principales
El an√°lisis PCA sobre las **3 fuentes** revela:

### Componente Principal (PC1) - 72% de la Varianza
- **Carga Factor Google Trends**: 0.45
- **Carga Factor Google Books**: 0.52  
- **Carga Factor Bain Usability**: 0.38
- **Interpretaci√≥n**: "Adopci√≥n General del Mercado"

### Componente Secundario (PC2) - 18% de la Varianza
- **Carga Factor Google Trends**: -0.23
- **Carga Factor Google Books**: 0.67
- **Carga Factor Bain Usability**: -0.12
- **Interpretaci√≥n**: "Inter√©s Acad√©mico vs. Comercial"

### Correlaciones Intersource
- **Google Trends ‚Üî Google Books**: r = 0.73 (fuerte)
- **Google Trends ‚Üî Bain Usability**: r = 0.68 (fuerte)
- **Google Books ‚Üî Bain Usability**: r = 0.81 (muy fuerte)""",
            "pca_analysis": f"""# An√°lisis PCA Detallado - {tool_name}

## Metodolog√≠a y Resultados
Se aplic√≥ an√°lisis de componentes principales a la matriz de correlaci√≥n de las 3 fuentes de datos.

### Varianza Explicada
- **PC1**: 72.3% (Adopci√≥n General)
- **PC2**: 18.1% (Divergencia Acad√©mica-Comercial)
- **PC3**: 9.6% (Ruido residual)

### Interpretaci√≥n de Componentes
1. **PC1 - "Adopci√≥n General"**: 
   - Representa la tendencia com√∫n de crecimiento en todas las fuentes
   - Fuerte loading positivo en Google Books (0.52)
   - Indica que el inter√©s acad√©mico lidera la adopci√≥n

2. **PC2 - "Divergencia Temporal"**:
   - Distingue entre fuentes con diferentes ritmos de adopci√≥n
   - Bain muestra adopci√≥n m√°s temprana que Google Trends
   - Sugiere diferentes ciclos de adopci√≥n por fuente

### Validez del Modelo
- **KMO**: 0.82 (excelente adecuaci√≥n muestral)
- **Test de Esfericidad de Bartlett**: p < 0.001 (significativo)
- **Comunalidades**: Todas > 0.65 (buena extracci√≥n)""",
            "heatmap_analysis": f"""# An√°lisis de Correlaci√≥n - Matriz de Calor

## Matriz de Correlaciones Intersource
```
                    GT    GB    BU
Google Trends     1.00  0.73  0.68
Google Books      0.73  1.00  0.81  
Bain Usability    0.68  0.81  1.00
```

## Patrones de Correlaci√≥n
- **Correlaci√≥n M√°s Fuerte**: Google Books ‚Üî Bain Usability (0.81)
- **Patr√≥n Temporal**: Todas las correlaciones son positivas y significativas
- **Consistencia**: No se detectaron correlaciones negativas

## An√°lisis de Clusters
Se identificaron **2 clusters principales**:
1. **Cluster Acad√©mico-Comercial**: Google Books + Bain Usability
2. **Cluster de Tendencias**: Google Trends (m√°s independiente)

## Outliers y Anomal√≠as
- **Puntos Outlier**: 2 casos con correlaciones at√≠picas
- **Anomal√≠as Temporales**: Per√≠odos de divergencia en Q2 2023""",
            "data_points_analyzed": 4521,
            "confidence_score": 0.92,
            "model_used": "gpt-4",
            "analysis_type": "multi_source",
            "tool_display_name": tool_name,
        }

        # Store the multi-source analysis
        print("üíæ Storing multi-source analysis in database...")
        record_id = precomputed_db.store_precomputed_analysis(
            combination_hash=combination_hash,
            tool_name=tool_name,
            selected_sources=selected_sources,
            language=language,
            analysis_data=simulated_multi_source_result,
        )
        print(f"‚úÖ Stored multi-source analysis record ID: {record_id}")

        # Retrieve and verify
        print("üîç Retrieving multi-source analysis...")
        retrieved_analysis = precomputed_db.get_combination_by_hash(combination_hash)

        if retrieved_analysis:
            print("‚úÖ Successfully retrieved multi-source analysis")
            print(f"   - Analysis Type: {retrieved_analysis['analysis_type']}")
            print(
                f"   - Principal Findings Length: {len(retrieved_analysis['principal_findings'])}"
            )
            print(
                f"   - PCA Analysis Length: {len(retrieved_analysis['pca_analysis'])}"
            )

            # Verify multi-source specific content
            multi_source_content = (
                "principal_findings" in retrieved_analysis
                and retrieved_analysis["principal_findings"]
                and "pca_analysis" in retrieved_analysis
                and retrieved_analysis["pca_analysis"]
                and retrieved_analysis["analysis_type"] == "multi_source"
            )

            print(
                f"   - Multi-source Content: {'‚úÖ PASSED' if multi_source_content else '‚ùå FAILED'}"
            )

            return {
                "success": True,
                "record_id": record_id,
                "multi_source_content": multi_source_content,
                "analysis_type": retrieved_analysis["analysis_type"],
            }
        else:
            return {"success": False, "error": "Multi-source retrieval failed"}

    except Exception as e:
        print(f"‚ùå Multi-source test failed: {e}")
        return {"success": False, "error": str(e)}


def test_performance_comparison():
    """Test performance: AI generation vs database retrieval."""
    print("\n‚ö° Performance Comparison Test")
    print("=" * 40)

    try:
        precomputed_db = get_precomputed_db_manager()

        # Test parameters
        tool_name = "Benchmarking"
        selected_sources = ["Google Trends", "Bain Usability"]
        language = "es"

        # Test database lookup speed
        print("üóÑÔ∏è Testing database lookup performance...")
        combination_hash = precomputed_db.generate_combination_hash(
            tool_name, selected_sources, language
        )

        start_time = time.time()
        for i in range(100):
            result = precomputed_db.get_combination_by_hash(combination_hash)
        end_time = time.time()

        db_avg_time = ((end_time - start_time) / 100) * 1000  # Convert to milliseconds
        print(f"‚úÖ Database lookup average: {db_avg_time:.2f}ms (100 iterations)")

        # Estimate AI generation time (this would be much slower in reality)
        estimated_ai_time = 8500  # Estimated 8.5 seconds for AI generation
        print(f"üìä Estimated AI generation time: {estimated_ai_time}ms")

        speed_improvement = estimated_ai_time / db_avg_time
        print(f"üöÄ Speed improvement: {speed_improvement:.0f}x faster")
        print(f"üéØ Performance target: <100ms (achieved: {db_avg_time:.2f}ms)")

        return {
            "db_avg_time": db_avg_time,
            "estimated_ai_time": estimated_ai_time,
            "speed_improvement": speed_improvement,
            "target_met": db_avg_time < 100,
        }

    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")
        return {"success": False, "error": str(e)}


def run_ai_integration_tests():
    """Run complete AI integration test suite."""
    print("üöÄ Starting AI Integration Tests - Phase 2")
    print("=" * 60)

    results = {}

    # Test 1: Single Source Analysis
    results["single_source"] = test_ai_integration_single_source()

    # Test 2: Multi-Source Analysis
    results["multi_source"] = test_ai_integration_multi_source()

    # Test 3: Performance Comparison
    results["performance"] = test_performance_comparison()

    # Summary
    print("\n" + "=" * 60)
    print("üéØ AI INTEGRATION TEST RESULTS SUMMARY")
    print("=" * 60)

    single_success = results["single_source"]["success"]
    multi_success = results["multi_source"]["success"]
    perf_target_met = results["performance"].get("target_met", False)

    print(f"‚úÖ Single Source Analysis: {'PASSED' if single_success else 'FAILED'}")
    print(f"‚úÖ Multi-Source Analysis: {'PASSED' if multi_success else 'FAILED'}")
    print(f"‚úÖ Performance Test: {'PASSED' if perf_target_met else 'FAILED'}")

    overall_success = single_success and multi_success and perf_target_met
    print(
        f"\nüéØ Overall Result: {'‚úÖ ALL TESTS PASSED' if overall_success else '‚ùå SOME TESTS FAILED'}"
    )

    if overall_success:
        print("\nüéâ AI Integration is ready for Phase 3: Full Precomputation Pipeline!")
    else:
        print("\n‚ö†Ô∏è  Review failed tests before proceeding to Phase 3")

    return results


if __name__ == "__main__":
    results = run_ai_integration_tests()
    sys.exit(0 if all(r.get("success", False) for r in results.values()) else 1)
