"""
Prompt Engineering System

Creates sophisticated prompts for doctoral-level analysis of
management tools data with emphasis on PCA insights and bilingual support.
"""

import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime


class PromptEngineer:
    """
    Creates sophisticated prompts for doctoral-level analysis.

    Generates context-aware prompts with PCA emphasis, bilingual support,
    and structured output requirements for AI analysis.
    """

    def __init__(self, language: str = "es"):
        """
        Initialize prompt engineer.

        Args:
            language: Analysis language ('es' or 'en')
        """
        self.language = language
        self.prompt_templates = self._load_templates()

    def create_narrative_analysis_prompt(
        self, data: Dict[str, Any], context: Dict[str, Any]
    ) -> str:
        """
        Create comprehensive analysis prompt.

        Args:
            data: Aggregated analysis data
            context: Additional context for analysis

        Returns:
            Complete analysis prompt string
        """
        import time

        start_time = time.time()
        logging.info(
            f"üìù Starting prompt generation for tool '{data.get('tool_name', 'Unknown')}' in {self.language}"
        )

        template = self.prompt_templates["comprehensive_analysis"][self.language]

        # Extract key information
        tool_name = data.get("tool_name", "Unknown Tool")
        sources = data.get("selected_sources", [])
        pca_insights = data.get("pca_insights", {})
        stats_summary = data.get("statistical_summary", {})
        trends = data.get("trends_analysis", {})
        data_quality = data.get("data_quality", {})
        heatmap_data = data.get("heatmap_analysis", {})

        # Build prompt sections
        sections = []

        # Context section
        sections.append(self._build_context_section(tool_name, sources, data))

        # Heatmap analysis section
        sections.append(self._build_heatmap_section(heatmap_data))

        # PCA emphasis section
        sections.append(self._build_pca_section(pca_insights))

        # Statistical analysis section
        sections.append(self._build_statistics_section(stats_summary))

        # Trends and patterns section
        sections.append(self._build_trends_section(trends))

        # Data quality section
        sections.append(self._build_data_quality_section(data_quality))

        # Analysis requirements
        sections.append(self._build_requirements_section())

        # Output format
        sections.append(self._build_output_format_section())

        prompt = template.format(
            analysis_date=datetime.now().strftime("%Y-%m-%d"),
            context="\n\n".join(sections),
        )

        generation_time = time.time() - start_time
        logging.info(
            f"‚úÖ Prompt generation completed in {generation_time:.2f}s - prompt length: {len(prompt)} characters"
        )
        logging.info(f"üìä Prompt sections created: {len(sections)} sections")

        return prompt

    def create_pca_focused_prompt(
        self, pca_data: Dict[str, Any], context: Dict[str, Any]
    ) -> str:
        """
        Create PCA-focused analysis prompt.

        Args:
            pca_data: PCA analysis data
            context: Additional context for analysis

        Returns:
            PCA-focused analysis prompt string
        """
        template = self.prompt_templates["pca_focused"][self.language]

        tool_name = context.get("tool_name", "Unknown Tool")
        components = pca_data.get("dominant_patterns", [])
        variance_explained = pca_data.get("total_variance_explained", 0)

        sections = []

        # PCA context
        sections.append(
            f"## Herramienta de Gesti√≥n Analizada: {tool_name}"
            if self.language == "es"
            else f"## Management Tool Analyzed: {tool_name}"
        )

        # Component analysis
        for i, component in enumerate(components[:3]):  # Top 3 components
            sections.append(self._build_component_analysis(component, i + 1))

        # Variance explanation
        sections.append(self._build_variance_analysis(variance_explained))

        # Interpretation requirements
        sections.append(self._build_pca_requirements())

        return template.format(
            analysis_date=datetime.now().strftime("%Y-%m-%d"),
            pca_analysis="\n\n".join(sections),
        )

    def create_executive_summary_prompt(self, findings: Dict[str, Any]) -> str:
        """
        Create prompt for executive summary generation.

        Args:
            findings: Analysis findings to summarize

        Returns:
            Executive summary prompt string
        """
        template = self.prompt_templates["executive_summary"][self.language]

        tool_name = findings.get("tool_name", "Unknown Tool")
        principal_findings = findings.get("principal_findings", [])

        sections = []

        # Executive context
        sections.append(
            f"## Herramienta: {tool_name}"
            if self.language == "es"
            else f"## Tool: {tool_name}"
        )

        # Key findings synthesis
        sections.append(self._build_findings_synthesis(principal_findings))

        # Strategic implications
        sections.append(self._build_strategic_implications(findings))

        # Recommendations
        sections.append(self._build_recommendations(findings))

        return template.format(
            executive_date=datetime.now().strftime("%Y-%m-%d"),
            executive_content="\n\n".join(sections),
        )

    def create_single_source_prompt(
        self, data: Dict[str, Any], context: Dict[str, Any]
    ) -> str:
        """
        Create single source analysis prompt with temporal, seasonal, and Fourier series analysis.

        Args:
            data: Aggregated analysis data from a single source
            context: Additional context for analysis

        Returns:
            Single source analysis prompt string
        """
        import time

        start_time = time.time()
        logging.info(
            f"üìù Starting single source prompt generation for tool '{data.get('tool_name', 'Unknown')}' in {self.language}"
        )

        template = self.prompt_templates["single_source_analysis"][self.language]

        # Extract key information
        tool_name = data.get("tool_name", "Unknown Tool")
        source_name = data.get("source_name", "Unknown Source")
        temporal_metrics = data.get("temporal_metrics", {})
        seasonal_patterns = data.get("seasonal_patterns", {})
        fourier_analysis = data.get("fourier_analysis", {})
        summary_statistics = data.get("summary_statistics", {})
        visualization_attributes = data.get("visualization_attributes", {})

        # Build prompt sections
        sections = []

        # Context section
        sections.append(
            self._build_single_source_context_section(tool_name, source_name, data)
        )

        # Executive Summary section
        sections.append(
            self._build_executive_summary_section(
                temporal_metrics, seasonal_patterns, fourier_analysis
            )
        )

        # Temporal Analysis section
        sections.append(
            self._build_temporal_analysis_section(temporal_metrics, summary_statistics)
        )

        # Seasonal Analysis section
        sections.append(
            self._build_seasonal_analysis_section(
                seasonal_patterns, visualization_attributes
            )
        )

        # Fourier Series Analysis section
        sections.append(
            self._build_fourier_analysis_section(
                fourier_analysis, visualization_attributes
            )
        )

        # Analysis requirements
        sections.append(self._build_single_source_requirements_section())

        # Output format
        sections.append(self._build_single_source_output_format_section())

        prompt = template.format(
            analysis_date=datetime.now().strftime("%Y-%m-%d"),
            context="\n\n".join(sections),
        )

        generation_time = time.time() - start_time
        logging.info(
            f"‚úÖ Single source prompt generation completed in {generation_time:.2f}s - prompt length: {len(prompt)} characters"
        )
        logging.info(f"üìä Prompt sections created: {len(sections)} sections")

        return prompt

    def _build_context_section(
        self, tool_name: str, sources: List[str], data: Dict[str, Any]
    ) -> str:
        """Build context section of prompt."""
        date_range = f"del {data.get('date_range_start', 'N/A')} al {data.get('date_range_end', 'N/A')}"
        data_points = data.get("data_points_analyzed", 0)

        if self.language == "es":
            return f"""
### CONTEXTO DEL AN√ÅLISIS

**Herramienta de Gesti√≥n:** {tool_name}
**Fuentes de Datos Seleccionadas:** {", ".join(sources)}
**Rango Temporal:** {date_range}
**Puntos de Datos Analizados:** {data_points:,}

Este an√°lisis se basa en datos multi-fuente recopilados de diversas bases de datos acad√©micas y empresariales,
proporcionando una visi√≥n integral del comportamiento de la herramienta de gesti√≥n a lo largo del tiempo.
"""
        else:
            return f"""
### ANALYSIS CONTEXT

**Management Tool:** {tool_name}
**Selected Data Sources:** {", ".join(sources)}
**Time Range:** {date_range}
**Data Points Analyzed:** {data_points:,}

This analysis is based on multi-source data collected from various academic and business databases,
providing a comprehensive view of the management tool's behavior over time.
"""

    def _build_pca_section(self, pca_insights: Dict[str, Any]) -> str:
        """Build PCA emphasis section with unified narrative prompt."""
        if not pca_insights or pca_insights.get("error"):
            return ""

        components = pca_insights.get("dominant_patterns", [])
        variance_explained = pca_insights.get("total_variance_explained", 0)
        tool_name = pca_insights.get("tool_name", "Unknown Tool")

        # Extract variable relationships for narrative
        variable_relationships = self._extract_variable_relationships(pca_insights)

        # Check for data quality issues
        sources_count = len(components[0].get("loadings", {})) if components else 0
        has_quality_issues = variance_explained < 10 or sources_count < 2

        # Build detailed PCA analysis with specific numerical insights
        detailed_pca_analysis = self._build_detailed_pca_narrative(
            components, tool_name, variance_explained
        )

        if self.language == "es":
            section = f"""
### AN√ÅLISIS DE COMPONENTES PRINCIPALES (PCA) - NARRATIVA UNIFICADA

**Datos PCA Adjuntos:**
- Herramienta de Gesti√≥n Analizada: {tool_name}
- Varianza Total Explicada: {variance_explained:.1f}%
- Componentes Principales Identificados: {len(components)}
- Fuentes de Datos Disponibles: {sources_count}

{detailed_pca_analysis}

**INSTRUCCIONES ESPEC√çFICAS PARA AN√ÅLISIS PCA DETALLADO:**

Basado en los datos num√©ricos anteriores, genera una narrativa unificada que:

1. **Interprete las cargas espec√≠ficas**: Usa los valores num√©ricos exactos (ej: "Google Trends con carga de +0.45")
2. **Explique las relaciones de oposici√≥n**: Cuando una fuente tiene carga positiva y otra negativa, explica esta tensi√≥n
3. **Conecte con la teor√≠a de gesti√≥n**: Relaciona los patrones con conceptos acad√©micos como "brecha teor√≠a-pr√°ctica"
4. **Use el porcentaje de varianza**: Menciona espec√≠ficamente "los primeros dos componentes explican el XX.X% de la varianza"
5. **Genere insights ejecutivos**: Traduce los hallazgos t√©cnicos implicaciones pr√°cticas para negocios

**Ejemplo del Formato Esperado:**
"Este PCA es particularmente poderoso porque sus primeros dos componentes (los ejes horizontal y vertical) capturan y explican un XX.X% combinado de la varianza total en los datos. Esto proporciona una narrativa clara y unificada sobre el viaje peligroso que una metodolog√≠a de gesti√≥n como {tool_name} toma desde la teor√≠a acad√©mica hasta la pr√°ctica industrial, destacando la brecha cr√≠tica entre teor√≠a y pr√°ctica.

El an√°lisis primero revela una 'din√°mica de adopci√≥n'. El inter√©s p√∫blico en {tool_name} (Google Trends) y la facilidad de uso percibida de sus herramientas (Bain - Usabilidad) est√°n estrechamente correlacionados, ambos mostrando fuerte influencia positiva a lo largo de los ejes de componentes principales. Por ejemplo, Google Trends tiene una carga positiva fuerte de aproximadamente +0.XX en el eje horizontal principal (PC1). Esto confirma num√©ricamente que a medida que {tool_name} se empaqueta en marcos accesibles, gana tracci√≥n en el mundo empresarial, un patr√≥n cl√°sico descrito en modelos acad√©micos de difusi√≥n de innovaci√≥n.

Sin embargo, esta popularidad crea una trampa. El PCA revela una relaci√≥n inversa poderosa: Bain - Satisfacci√≥n aparece en oposici√≥n directa a esta tendencia de crecimiento, con una carga negativa fuerte de aproximadamente -0.XX en PC1. Este contraste num√©rico stark visualiza un modo de falla cr√≠tico. A medida que el impulso por herramientas simplificadas y populares impulsa la din√°mica en una direcci√≥n (positiva en PC1), la satisfacci√≥n se jala en la direcci√≥n completamente opuesta. Desde una perspectiva acad√©mica, esto es un fracaso de fidelidad de implementaci√≥n; para l√≠deres industriales, es una advertencia respaldada por datos de que adoptar los aspectos superficiales de {tool_name} lleva a un fracaso predecible.

Finalmente, el an√°lisis muestra que el discurso acad√©mico riguroso sobre {tool_name} (Crossref.org) opera en un eje de influencia completamente diferente. Tiene la carga individual m√°s alta en el eje vertical (+0.XX en PC2) mientras est√° negativamente asociado con el eje de tendencia principal (-0.XX en PC1). Esta posici√≥n perpendicular confirma num√©ricamente que la conversaci√≥n acad√©mica est√° desconectada del ciclo de hype de practicantes. El verdadero √©xito, sugiere el gr√°fico, radica en conectar estos mundos‚Äîusando principios rigurosos para informar la pr√°ctica en lugar de simplemente seguir una tendencia popular que lleva a la insatisfacci√≥n."

"""
        else:
            section = f"""
### PRINCIPAL COMPONENT ANALYSIS (PCA) - UNIFIED NARRATIVE

**Attached PCA Data:**
- Management Tool Analyzed: {tool_name}
- Total Variance Explained: {variance_explained:.1f}%
- Principal Components Identified: {len(components)}
- Data Sources Available: {sources_count}

"""

            # Add specific guidance for low-quality data scenarios
            if has_quality_issues:
                section += f"""
**‚ö†Ô∏è IMPORTANT NOTE: LIMITED DATA QUALITY**

The current analysis shows significant limitations:
- Very low variance explained ({variance_explained:.1f}%)
- {sources_count} data source(s) available

**Specific Instructions for This Scenario:**
1. **Focus on identifying data problems** rather than patterns
2. **Suggest specific improvements** for data quality
3. **Recommend additional sources** that could enrich the analysis
4. **Provide strategic insights** based on current limitations
5. **Be honest about limitations** but provide executive value

**Example of Expected Analysis:**
"The current PCA analysis is limited by {sources_count} data source(s), explaining only {variance_explained:.1f}% of variance. This suggests the need to incorporate additional sources like [suggest specific sources] for a more comprehensive view. Meanwhile, available data indicates [extract any possible insight]..."

"""

            # Build detailed PCA analysis with specific numerical insights
            detailed_pca_analysis = self._build_detailed_pca_narrative(
                components, tool_name, variance_explained
            )

            # Continue with regular PCA instructions
            section += f"""
{detailed_pca_analysis}

**SPECIFIC INSTRUCTIONS FOR DETAILED PCA ANALYSIS:**

Based on the numerical data above, generate a unified narrative that:

1. **Interprets specific loadings**: Use exact numerical values (e.g., "Google Trends with loading of +0.45")
2. **Explains opposition relationships**: When one source has positive and another negative loading, explain this tension
3. **Connects with management theory**: Relate patterns to academic concepts like "theory-practice gap"
4. **Uses variance percentage**: Specifically mention "the first two components explain XX.X% of variance"
5. **Generates executive insights**: Translate technical findings into practical business implications

**Expected Format Example:**
"This PCA is particularly powerful because its first two components (the horizontal and vertical axes) capture and explain a combined XX.X% of the total variance in the data. This provides a clear, unified narrative about the perilous journey a management methodology like {tool_name} takes from academic theory to industry practice, highlighting the critical theory-practice gap.

The analysis first reveals an 'adoption dynamic.' The public interest in {tool_name} (Google Trends) and the perceived ease-of-use of its tools (Bain - Usabilidad) are closely correlated, both showing strong positive influence along the principal component axes. For instance, Google Trends has a strong positive loading of approximately +0.XX on the main horizontal axis (PC1). This numerically confirms that as {tool_name} is packaged into accessible frameworks, it gains traction in the business world, a classic pattern described in academic models of innovation diffusion.

However, this popularity creates a trap. The PCA reveals a powerful inverse relationship: Bain - Satisfacci√≥n appears in direct opposition to this growth trend, with a strong negative loading of approximately -0.XX on PC1. This stark numerical contrast visualizes a critical failure mode. As the push for simplified, popular tools drives the dynamic in one direction (positive on PC1), satisfaction is pulled in the complete opposite direction. From an academic view, this is a failure of implementation fidelity; for industry leaders, it's a data-backed warning that adopting the superficial aspects of {tool_name} leads to predictable failure.

Finally, the analysis shows that the rigorous academic discourse on {tool_name} (Crossref.org) operates on an entirely different axis of influence. It has the single highest loading on the vertical axis (+0.XX on PC2) while being negatively associated with the main trend axis (-0.XX on PC1). This perpendicular position numerically confirms that the academic conversation is disconnected from the practitioner hype cycle. True success, the chart suggests, lies in bridging these worlds‚Äîusing rigorous principles to inform practice rather than simply following a popular trend that leads to dissatisfaction."

"""

        for i, component in enumerate(components[:3]):
            comp_num = i + 1
            interpretation = component.get("interpretation", f"Component {comp_num}")
            variance = component.get("variance_explained", 0)
            loadings = component.get("loadings", {})

            if self.language == "es":
                section += f"""
**Componente {comp_num}** ({variance:.1f}% varianza explicada):
{interpretation}
"""
                if loadings:
                    section += "**Cargas principales:**\n"
                    for var, loading in loadings.items():
                        section += f"- {var}: {loading:.3f}\n"
            else:
                section += f"""
**Component {comp_num}** ({variance:.1f}% variance explained):
{interpretation}
"""
                if loadings:
                    section += "**Principal loadings:**\n"
                    for var, loading in loadings.items():
                        section += f"- {var}: {loading:.3f}\n"

        return section

    def _build_statistics_section(self, stats_summary: Dict[str, Any]) -> str:
        """Build statistical analysis section."""
        if not stats_summary:
            return ""

        source_stats = stats_summary.get("source_statistics", {})
        correlations = stats_summary.get("correlations", {})

        if self.language == "es":
            section = """
### AN√ÅLISIS ESTAD√çSTICO COMPRENSIVO

**Estad√≠sticas por Fuente de Datos:**
"""
        else:
            section = """
### COMPREHENSIVE STATISTICAL ANALYSIS

**Statistics by Data Source:**
"""

        # Add source statistics
        for source, stats in source_stats.items():
            if self.language == "es":
                section += f"""
**{source}:**
- Media: {stats.get("mean", "N/A"):.2f}
- Desviaci√≥n Est√°ndar: {stats.get("std", "N/A"):.2f}
- Tendencia: {stats.get("trend", {}).get("trend_direction", "N/A")}
- Significancia: {stats.get("trend", {}).get("significance", "N/A")}
"""
            else:
                section += f"""
**{source}:**
- Mean: {stats.get("mean", "N/A"):.2f}
- Standard Deviation: {stats.get("std", "N/A"):.2f}
- Trend: {stats.get("trend", {}).get("trend_direction", "N/A")}
- Significance: {stats.get("trend", {}).get("significance", "N/A")}
"""

        # Add correlations
        if correlations:
            if self.language == "es":
                section += "\n**Correlaciones Significativas Entre Fuentes:**\n"
            else:
                section += "\n**Significant Correlations Between Sources:**\n"

            for corr_pair, corr_data in correlations.items():
                if corr_data.get("significance") == "significant":
                    strength = corr_data.get("strength", "unknown")
                    if self.language == "es":
                        section += f"- {corr_pair}: Correlaci√≥n {strength} ({corr_data.get('correlation', 0):.3f})\n"
                    else:
                        section += f"- {corr_pair}: {strength} correlation ({corr_data.get('correlation', 0):.3f})\n"

        return section

    def _build_trends_section(self, trends: Dict[str, Any]) -> str:
        """Build trends and patterns section with emphasis on integration."""
        if not trends:
            return ""

        trend_data = trends.get("trends", {})
        anomalies = trends.get("anomalies", {})
        patterns = trends.get("overall_patterns", [])

        if self.language == "es":
            section = """
### AN√ÅLISIS TEMPORAL INTEGRADO PARA HALLAZGOS PRINCIPALES

**Datos Temporales para Integrar en Hallazgos Principales:**

**INSTRUCCI√ìN ESPEC√çFICA:** Estos datos temporales DEBEN ser integrados en la secci√≥n "Hallazgos Principales" como narrativa fluida, NO como vi√±etas. Conecte los patrones temporales con los hallazgos de PCA.

**Tendencias Temporales Clave:**
"""
        else:
            section = """
### INTEGRATED TEMPORAL ANALYSIS FOR PRINCIPAL FINDINGS

**Temporal Data to Integrate into Principal Findings:**

**SPECIFIC INSTRUCTION:** This temporal data MUST be integrated into the "Principal Findings" section as fluid narrative, NOT as bullet points. Connect temporal patterns with PCA findings.

**Key Temporal Trends:**
"""

        # Add trend information with integration guidance
        for source, trend_info in trend_data.items():
            direction = trend_info.get("trend_direction", "stable")
            momentum = trend_info.get("momentum", 0)
            volatility = trend_info.get("volatility", 0)

            if self.language == "es":
                section += f"""
**{source}:** tendencia {direction} con momento de {momentum:.3f} y volatilidad de {volatility:.3f}
"""
                # Add integration guidance
                if direction in ["strong_upward", "moderate_upward"]:
                    section += f"‚Üí Integrar este crecimiento con cargas PCA positivas de {source}\n"
                elif direction in ["strong_downward", "moderate_downward"]:
                    section += f"‚Üí Conectar esta disminuci√≥n con posibles cargas PCA negativas\n"
                else:
                    section += (
                        f"‚Üí Analizar estabilidad de {source} en contexto multivariado\n"
                    )
            else:
                section += f"""
**{source}:** {direction} trend with momentum of {momentum:.3f} and volatility of {volatility:.3f}
"""
                # Add integration guidance
                if direction in ["strong_upward", "moderate_upward"]:
                    section += f"‚Üí Integrate this growth with positive PCA loadings of {source}\n"
                elif direction in ["strong_downward", "moderate_downward"]:
                    section += (
                        f"‚Üí Connect this decline with possible negative PCA loadings\n"
                    )
                else:
                    section += (
                        f"‚Üí Analyze stability of {source} in multivariate context\n"
                    )

        # Add anomalies with integration guidance
        if anomalies:
            if self.language == "es":
                section += "\n**Anomal√≠as Temporales para An√°lisis:**\n"
                section += "**INSTRUCCI√ìN:** Conecte estas anomal√≠as con patrones PCA inesperados\n\n"
            else:
                section += "\n**Temporal Anomalies for Analysis:**\n"
                section += "**INSTRUCTION:** Connect these anomalies with unexpected PCA patterns\n\n"

            for source, anomaly_info in anomalies.items():
                count = anomaly_info.get("count", 0)
                percentage = anomaly_info.get("percentage", 0)
                max_z = anomaly_info.get("max_z_score", 0)

                if self.language == "es":
                    section += f"- {source}: {count} anomal√≠as ({percentage:.1f}%), Z-score m√°ximo: {max_z:.2f}\n"
                    section += f"  ‚Üí Analizar c√≥mo estas anomal√≠as afectan las relaciones PCA\n"
                else:
                    section += f"- {source}: {count} anomalies ({percentage:.1f}%), Max Z-score: {max_z:.2f}\n"
                    section += (
                        f"  ‚Üí Analyze how these anomalies affect PCA relationships\n"
                    )

        # Add overall patterns with integration guidance
        if patterns:
            if self.language == "es":
                section += "\n**Patrones Temporales Generales para Integraci√≥n:**\n"
                section += "**INSTRUCCI√ìN:** Use estos patrones para enriquecer la narrativa de Hallazgos Principales\n\n"
            else:
                section += "\n**Overall Temporal Patterns for Integration:**\n"
                section += "**INSTRUCTION:** Use these patterns to enrich the Principal Findings narrative\n\n"

            for pattern in patterns:
                section += f"- {pattern}\n"
                if self.language == "es":
                    section += f"  ‚Üí Conectar este patr√≥n con la din√°mica de componentes principales\n"
                else:
                    section += (
                        f"  ‚Üí Connect this pattern with principal component dynamics\n"
                    )

        return section

    def _build_heatmap_section(self, heatmap_data: Dict[str, Any]) -> str:
        """Build heatmap analysis section."""
        if not heatmap_data:
            return ""

        # Extract heatmap metrics
        value_ranges = heatmap_data.get("value_ranges", {})
        dense_regions = heatmap_data.get("most_dense_regions", [])
        sparse_regions = heatmap_data.get("least_dense_regions", [])
        clusters = heatmap_data.get("detected_clusters", [])
        outliers = heatmap_data.get("detected_outliers", [])
        gradients = heatmap_data.get("gradients", {})

        if self.language == "es":
            section = """
### AN√ÅLISIS DEL MAPA DE CALOR

**Datos del Mapa de Calor Proporcionados:**
"""
        else:
            section = """
### HEATMAP ANALYSIS

**Provided Heatmap Data:**
"""

        # Add value ranges
        if value_ranges:
            if self.language == "es":
                section += "\n**Rangos de Valores del Mapa de Calor:**\n"
            else:
                section += "\n**Heatmap Value Ranges:**\n"

            for source, ranges in value_ranges.items():
                min_val = ranges.get("min", "N/A")
                max_val = ranges.get("max", "N/A")
                if self.language == "es":
                    section += f"- {source}: m√≠nimo {min_val}, m√°ximo {max_val}\n"
                else:
                    section += f"- {source}: min {min_val}, max {max_val}\n"

        # Add dense regions
        if dense_regions:
            if self.language == "es":
                section += "\n**Regiones M√°s Densas:**\n"
            else:
                section += "\n**Most Dense Regions:**\n"

            for region in dense_regions:
                section += f"- {region}\n"

        # Add sparse regions
        if sparse_regions:
            if self.language == "es":
                section += "\n**Regiones Menos Densas:**\n"
            else:
                section += "\n**Least Dense Regions:**\n"

            for region in sparse_regions:
                section += f"- {region}\n"

        # Add detected clusters
        if clusters:
            if self.language == "es":
                section += "\n**Agrupamientos Detectados:**\n"
            else:
                section += "\n**Detected Clusters:**\n"

            for cluster in clusters:
                section += f"- {cluster}\n"

        # Add detected outliers
        if outliers:
            if self.language == "es":
                section += "\n**Valores At√≠picos Detectados:**\n"
            else:
                section += "\n**Detected Outliers:**\n"

            for outlier in outliers:
                section += f"- {outlier}\n"

        # Add gradients
        if gradients:
            if self.language == "es":
                section += "\n**Gradientes Observados:**\n"
            else:
                section += "\n**Observed Gradients:**\n"

            for gradient_type, description in gradients.items():
                section += f"- {gradient_type}: {description}\n"

        # Add analysis instructions
        if self.language == "es":
            section += """

**INSTRUCCIONES OBLIGATORIAS PARA AN√ÅLISIS DEL MAPA DE CALOR:**

Basado en los datos proporcionados arriba, analiza el mapa de calor y discute:

1. **Patrones Visuales Clave**: Identifica los patrones m√°s prominentes en la visualizaci√≥n
2. **Agrupamientos**: Describe cualquier cluster o agrupamiento visible y sus caracter√≠sticas
3. **Anomal√≠as**: Identifica valores at√≠picos o anomal√≠as y explica su significado
4. **Gradientes**: Analiza los gradientes de color y qu√© representan en t√©rminos de intensidad de datos
5. **Implicaciones para el Conjunto de Datos**: Explica c√≥mo estos patrones afectan la interpretaci√≥n general de los datos

**Enfoque del An√°lisis:**
- Conecta los patrones del mapa de calor con las tendencias temporales
- Relaciona los clusters con los hallazgos de PCA cuando sea relevante
- Identifica √°reas de alta densidad que puedan indicar per√≠odos de inter√©s significativo
- Explica anomal√≠as en el contexto del comportamiento general de la herramienta de gesti√≥n

**REQUISITO OBLIGATORIO PARA EL FORMATO DE SALIDA:**
- Esta secci√≥n proporciona los datos y instrucciones para que generes el contenido de la secci√≥n "heatmap_analysis" en el JSON de salida
- **DEBES generar el campo "heatmap_analysis" en tu respuesta JSON**
- **DEBES crear un an√°lisis de EXACTAMENTE 3 p√°rrafos separados por \n\n**
- **Si no hay datos de heatmap disponibles, crea un an√°lisis basado en correlaciones generales**
- **El campo heatmap_analysis es OBLIGATORIO - no lo omitas bajo ninguna circunstancia**
"""
        else:
            section += """

**MANDATORY HEATMAP ANALYSIS INSTRUCTIONS:**

Based on the data provided above, analyze the heatmap and discuss:

1. **Key Visual Patterns**: Identify the most prominent patterns in the visualization
2. **Clusters**: Describe any visible clusters or groupings and their characteristics
3. **Anomalies**: Identify outliers or anomalies and explain their significance
4. **Gradients**: Analyze color gradients and what they represent in terms of data intensity
5. **Implications for the Dataset**: Explain how these patterns affect the overall interpretation of the data

**Analysis Focus:**
- Connect heatmap patterns with temporal trends
- Relate clusters with PCA findings when relevant
- Identify high-density areas that may indicate periods of significant interest
- Explain anomalies in the context of the management tool's general behavior

**MANDATORY OUTPUT REQUIREMENT:**
- This section provides the data and instructions for you to generate the content of the "heatmap_analysis" field in the output JSON
- **YOU MUST generate the "heatmap_analysis" field in your JSON response**
- **YOU MUST create an analysis of EXACTLY 3 paragraphs separated by \n\n**
- **If no heatmap data is available, create an analysis based on general correlations**
- **The heatmap_analysis field is MANDATORY - do not omit it under any circumstances**
"""

        return section

    def _build_data_quality_section(self, data_quality: Dict[str, Any]) -> str:
        """Build data quality assessment section."""
        if not data_quality:
            return ""

        overall_score = data_quality.get("overall_score", 0)
        completeness = data_quality.get("completeness", {})
        timeliness = data_quality.get("timeliness", {})

        if self.language == "es":
            section = f"""
### EVALUACI√ìN DE CALIDAD DE DATOS

**Puntuaci√≥n General de Calidad:** {overall_score:.1f}/100

**Completitud por Fuente:**
"""
        else:
            section = f"""
### DATA QUALITY ASSESSMENT

**Overall Quality Score:** {overall_score:.1f}/100

**Completeness by Source:**
"""

        # Add completeness information
        for source, comp_data in completeness.items():
            comp_pct = comp_data.get("completeness_percentage", 0)
            missing_pct = comp_data.get("missing_percentage", 0)

            if self.language == "es":
                section += f"- {source}: {comp_pct:.1f}% completo, {missing_pct:.1f}% faltante\n"
            else:
                section += f"- {source}: {comp_pct:.1f}% complete, {missing_pct:.1f}% missing\n"

        # Add timeliness
        if timeliness:
            latest_date = timeliness.get("latest_date", "N/A")
            days_since = timeliness.get("days_since_latest", 0)
            timeliness_score = timeliness.get("timeliness_score", 0)

            if self.language == "es":
                section += f"""
**Actualidad de los Datos:**
- Fecha m√°s reciente: {latest_date}
- D√≠as desde actualizaci√≥n: {days_since}
- Puntuaci√≥n de actualidad: {timeliness_score:.1f}/100
"""
            else:
                section += f"""
**Data Timeliness:**
- Most Recent Date: {latest_date}
- Days Since Update: {days_since}
- Timeliness Score: {timeliness_score:.1f}/100
"""

        return section

    def _build_requirements_section(self) -> str:
        """Build analysis requirements section."""
        if self.language == "es":
            return """
### REQUISITOS DEL AN√ÅLISIS

Por favor, proporciona un an√°lisis doctoral-level que:

1. **Sintetice Informaci√≥n Multi-fuente**: Integre insights de todas las fuentes de datos incluyendo an√°lisis temporal, de heatmap y PCA
2. **√ânfasis en An√°lisis de Mapa de Calor**: Destaque patrones visuales clave, clusters, anomal√≠as y gradientes del heatmap con explicaciones claras integradas en la narrativa
2. **√ânfasis en PCA**: Destaque insights de componentes principales con explicaciones claras integradas en una narrativa fluida
3. **Identifique Patrones Temporales**: Detecte tendencias, ciclos y anomal√≠as significativas e integrelas en los hallazgos principales
4. **Genere Conclusiones Ejecutivas**: Proporcione insights accionables para tomadores de decisiones
5. **Mantenga Rigor Acad√©mico**: Use terminolog√≠a apropiada y metodolog√≠a sistem√°tica
6. **Mencione la Herramienta Espec√≠fica**: Incluya el nombre de la herramienta de gesti√≥n analizada en todos los hallazgos para personalizar el an√°lisis

**ESTRUCTURA REQUERIDA DEL AN√ÅLISIS:**

Genera un an√°lisis doctoral con las siguientes cuatro secciones principales:

**1. Resumen Ejecutivo:**
- **REQUISITO MEJORADO**: Un p√°rrafo conciso pero completo que capture los insights m√°s cr√≠ticos
- **CONTENIDO ESENCIAL**: Debe incluir (1) el gap teor√≠a-pr√°ctica, (2) implicaciones estrat√©gicas, (3) tendencias temporales clave, (4) insights de heatmap, (5) patrones visuales del mapa de calor
- **DATOS CUANTITATIVOS**: Mencione espec√≠ficamente el porcentaje de varianza explicada por los primeros dos componentes y al menos 2 valores num√©ricos exactos
- **CONTEXTO ESPEC√çFICO**: Conecte los hallazgos con la herramienta de gesti√≥n espec√≠fica analizada
- **EJEMPLO DE CALIDAD**: "El an√°lisis de 'Herramienta X' revela una brecha cr√≠tica entre teor√≠a y pr√°ctica, con los primeros dos componentes explicando el XX.X% de la varianza. La tendencia temporal muestra [patr√≥n espec√≠fico] mientras que el an√°lisis de correlaci√≥n indica [insight espec√≠fico], sugiriendo [implicaci√≥n estrat√©gica]."

**2. Hallazgos Principales:**
- **REQUISITO ABSOLUTO**: M√öLTIPLES vi√±etas concisas y accionables (3-5 vi√±etas diferentes)
- **FORMATO OBLIGATORIO**: Cada vi√±eta debe comenzar con "‚Ä¢" o "-" y ser una l√≠nea separada
- Cada vi√±eta debe ser un hallazgo espec√≠fico y diferente con datos cuantitativos
- **REQUISITO DE CONTENIDO ESPEC√çFICO**: Debe incluir al menos una vi√±eta con an√°lisis temporal, una vi√±eta con insights de heatmap, y una vi√±eta con patrones visuales del mapa de calor
- Integre insights de PCA, an√°lisis temporal, y heatmap en cada vi√±eta
- Conecte los patrones temporales con los hallazgos de PCA en diferentes vi√±etas
- Mencione fuentes espec√≠ficas y valores num√©ricos exactos en cada vi√±eta
- **ADVERTENCIA CR√çTICA**: NO genere un solo p√°rrafo grande, genere varias vi√±etas distintas separadas por saltos de l√≠nea
- **EJEMPLO DE FORMATO CORRECTO**:
  ‚Ä¢ Hallazgo 1 con datos cuantitativos espec√≠ficos
  ‚Ä¢ Hallazgo 2 con an√°lisis temporal integrado (tendencias, ciclos, anomal√≠as)
  ‚Ä¢ Hallazgo 3 con insights de PCA
  ‚Ä¢ Hallazgo 4 con patr√≥n de correlaci√≥n/heatmap
  ‚Ä¢ Hallazgo 5 con conclusi√≥n estrat√©gica

**3. An√°lisis de Mapa de Calor:**
- **REQUISITO ABSOLUTO**: Un ensayo anal√≠tico de EXACTAMENTE 3 p√°rrafos separados por DOS l√≠neas en blanco
- **ADVERTENCIA CR√çTICA**: Si no genera exactamente 3 p√°rrafos distintos, el an√°lisis ser√° rechazado
- **P√°rrafo 1** (termina con la primera l√≠nea en blanco): Analice los patrones visuales clave, clusters y gradientes observados en el mapa de calor
- **P√°rrafo 2** (termina con la segunda l√≠nea en blanco): Interprete las anomal√≠as y valores at√≠picos detectados, explicando su significado en el contexto de los datos
- **P√°rrafo 3** (no necesita l√≠nea en blanco al final): Discuta las implicaciones de estos patrones para el conjunto de datos y su relaci√≥n con las tendencias temporales
- **ESTRUCTURA FORZADA**: P√°rrafo 1 + \n\n + P√°rrafo 2 + \n\n + P√°rrafo 3
- **VERIFICACI√ìN AUTOM√ÅTICA**: El sistema contar√° los p√°rrafos - debe haber exactamente 3
- Use los rangos de valores, regiones densas/espresas y clusters proporcionados
- Conecte con los hallazgos de PCA cuando sea relevante

**4. An√°lisis PCA:**
- **REQUISITO ABSOLUTO E INNEGOCIABLE**: Un ensayo anal√≠tico de EXACTAMENTE 3 p√°rrafos separados por DOS l√≠neas en blanco (NO datos estad√≠sticos)
- **ADVERTENCIA CR√çTICA**: Si no genera exactamente 3 p√°rrafos distintos, el an√°lisis ser√° rechazado
- **P√°rrafo 1** (termina con la primera l√≠nea en blanco): Interprete las cargas espec√≠ficas con valores num√©ricos exactos y explique las relaciones de oposici√≥n entre fuentes
- **P√°rrafo 2** (termina con la segunda l√≠nea en blanco): Analice las RELACIONES entre las diferentes fuentes de datos, enfoc√°ndose en c√≥mo interact√∫an y qu√© patrones revelan estas interacciones
- **P√°rrafo 3** (no necesita l√≠nea en blanco al final): Discuta las IMPLICACIONES estrat√©gicas y pr√°cticas de estos patrones para la implementaci√≥n y adopci√≥n de la herramienta de gesti√≥n
- **ESTRUCTURA FORZADA**: P√°rrafo 1 + \n\n + P√°rrafo 2 + \n\n + P√°rrafo 3
- **VERIFICACI√ìN AUTOM√ÅTICA**: El sistema contar√° los p√°rrafos - debe haber exactamente 3
- Conecte con conceptos acad√©micos como "brecha teor√≠a-pr√°ctica"
- Use el porcentaje de varianza explicada

**EJEMPLO ESTRUCTURAL OBLIGATORIO para heatmap_analysis:**
"Contenido del P√°rrafo 1 sobre patrones visuales, clusters y gradientes.\n\nContenido del P√°rrafo 2 sobre anomal√≠as y valores at√≠picos.\n\nContenido del P√°rrafo 3 sobre implicaciones para el conjunto de datos."

**EJEMPLO ESTRUCTURAL OBLIGATORIO para pca_analysis:**
"Contenido del P√°rrafo 1 sobre interpretaci√≥n t√©cnica con cargas espec√≠ficas.\n\nContenido del P√°rrafo 2 sobre relaciones entre fuentes de datos.\n\nContenido del P√°rrafo 3 sobre implicaciones estrat√©gicas y pr√°cticas."

**ADVERTENCIA**: El ejemplo anterior muestra EXACTAMENTE c√≥mo debe estructurarse con \n\n entre p√°rrafos.

**CR√çTICO: SOLO JSON ESTRICTO - REQUISITO OBLIGATORIO**
Debes responder √öNICAMENTE con JSON v√°lido. Sin explicaciones, sin markdown, sin texto adicional.

**REQUISITO ABSOLUTO: INCLUIR heatmap_analysis**
- El campo "heatmap_analysis" ES OBLIGATORIO en tu respuesta JSON
- Debes generar este campo incluso si no hay datos de heatmap disponibles
- Usa los datos proporcionados en la secci√≥n "AN√ÅLISIS DEL MAPA DE CALOR" para generar este contenido

**FORMATO OBLIGATORIO:**
Comienza tu respuesta con { y termina con }. Nada m√°s.

**ESTRUCTURA EXACTA REQUERIDA:**
{
  "executive_summary": "Escribe un p√°rrafo conciso sobre el an√°lisis de la herramienta de gesti√≥n",
  "principal_findings": [
    "‚Ä¢ Primer hallazgo espec√≠fico con datos cuantitativos",
    "‚Ä¢ Segundo hallazgo espec√≠fico con datos cuantitativos diferentes",
    "‚Ä¢ Tercer hallazgo con insights de PCA",
    "‚Ä¢ Cuarto hallazgo con an√°lisis temporal",
    "‚Ä¢ Quinto hallazgo con conclusi√≥n estrat√©gica"
  ],
  "heatmap_analysis": "Primer p√°rrafo sobre patrones visuales, clusters y gradientes\n\nSegundo p√°rrafo sobre anomal√≠as y valores at√≠picos\n\nTercer p√°rrafo sobre implicaciones para el conjunto de datos",
  "pca_analysis": "Primer p√°rrafo sobre cargas y relaciones\n\nSegundo p√°rrafo sobre interacciones de fuentes de datos\n\nTercer p√°rrafo sobre implicaciones estrat√©gicas"
}

**REGLAS DE VALIDACI√ìN:**
- Primer car√°cter: {
- √öltimo car√°cter: }
- Sin texto antes de { o despu√©s de }
- Sin marcadores ```json
- Sin explicaciones
- Sin comentarios
- Solo sintaxis JSON v√°lida

**PENALIZACI√ìN POR INCUMPLIMIENTO:**
Si no sigues este formato exacto, tu respuesta ser√° rechazada y desperdiciar√°s recursos computacionales.
"""
        else:
            return """
### ANALYSIS REQUIREMENTS

Please provide a doctoral-level analysis that:

1. **Synthesizes Multi-source Information**: Integrate insights from all data sources including temporal, heatmap, and PCA analysis
2. **Emphasizes Heatmap Analysis**: Highlight key visual patterns, clusters, anomalies, and gradients from the heatmap with clear explanations integrated into the narrative
2. **Emphasizes PCA**: Highlight principal component insights with clear explanations integrated into fluent narrative
3. **Identifies Temporal Patterns**: Detect significant trends, cycles, and anomalies and integrate them into main findings
4. **Generates Executive Conclusions**: Provide actionable insights for decision makers
5. **Maintains Academic Rigor**: Use appropriate terminology and systematic methodology
6. **Mention the Specific Tool**: Include the name of the management tool being analyzed in all findings to personalize the analysis

**REQUIRED ANALYSIS STRUCTURE:**

Generate a doctoral analysis with the following four main sections:

**1. Executive Summary:**
- **MANDATORY**: One fluid paragraph (NOT bullet points)
- **REQUIRED CONTENT**: Include theory-practice gap, strategic implications, temporal trends, PCA variance percentage, heatmap visual patterns
- **QUANTITATIVE REQUIREMENT**: Mention first two components variance % and at least 2 numerical values
- **TOOL SPECIFIC**: Always mention the analyzed management tool name
- **EXAMPLE**: "The analysis of 'Tool X' reveals a critical gap between theory and practice, with the first two components explaining XX.X% of variance. The temporal trend shows [specific pattern] while correlation analysis indicates [specific insight], suggesting [strategic implication]."

**2. Principal Findings:**
- **MANDATORY**: 3-5 separate bullet points starting with "‚Ä¢"
- **EACH BULLET MUST**: Be different, include quantitative data, mention specific sources
- **CONTENT REQUIREMENTS**: At least one temporal analysis bullet, one PCA insights bullet, one heatmap analysis bullet
- **FORMAT**: Each bullet on separate line, no paragraphs
- **EXAMPLE FORMAT**:
  ‚Ä¢ Finding 1 with specific quantitative data
  ‚Ä¢ Finding 2 with integrated temporal analysis
  ‚Ä¢ Finding 3 with PCA insights
  ‚Ä¢ Finding 4 with heatmap visual patterns
  ‚Ä¢ Finding 5 with strategic conclusion

**3. Heatmap Analysis:**
- **MANDATORY**: EXACTLY 3 paragraphs separated by \n\n
- **Paragraph 1**: Analysis of key visual patterns, clusters, and gradients observed in the heatmap
- **Paragraph 2**: Interpretation of detected anomalies and outliers, explaining their significance
- **Paragraph 3**: Discussion of implications for the dataset and relationship to temporal trends
- **STRICT FORMAT**: "Paragraph 1 content\n\nParagraph 2 content\n\nParagraph 3 content"
- **VERIFICATION**: System counts paragraphs - must be exactly 3
- Use provided value ranges, dense/sparse regions, and detected clusters
- Connect with PCA findings when relevant

**4. PCA Analysis:**
- **MANDATORY**: EXACTLY 3 paragraphs separated by \n\n
- **Paragraph 1**: Technical interpretation with specific loadings and relationships
- **Paragraph 2**: Analysis of relationships between data sources
- **Paragraph 3**: Strategic and practical implications
- **STRICT FORMAT**: "Paragraph 1 content\n\nParagraph 2 content\n\nParagraph 3 content"
- **VERIFICATION**: System counts paragraphs - must be exactly 3

**CRITICAL: STRICT JSON OUTPUT ONLY**
You MUST respond with VALID JSON only. No explanations, no markdown, no additional text.

**MANDATORY REQUIREMENT: INCLUDE heatmap_analysis**
- The "heatmap_analysis" field IS MANDATORY in your JSON response
- You must generate this field even if no heatmap data is available
- Use the data provided in the "HEATMAP ANALYSIS" section to generate this content

**MANDATORY FORMAT:**
Start your response with { and end with }. Nothing else.

**EXACT STRUCTURE REQUIRED:**
{
  "executive_summary": "Write a concise paragraph about the management tool analysis",
  "principal_findings": [
    "‚Ä¢ First specific finding with quantitative data",
    "‚Ä¢ Second specific finding with different quantitative data",
    "‚Ä¢ Third finding with PCA insights",
    "‚Ä¢ Fourth finding with temporal analysis",
    "‚Ä¢ Fifth finding with strategic conclusion"
  ],
  "heatmap_analysis": "First paragraph about visual patterns, clusters, and gradients\n\nSecond paragraph about anomalies and outliers\n\nThird paragraph about implications for the dataset",
  "pca_analysis": "First paragraph about loadings and relationships\n\nSecond paragraph about data source interactions\n\nThird paragraph about strategic implications"
}

**VALIDATION RULES:**
- First character: {
- Last character: }
- No text before { or after }
- No ```json markers
- No explanations
- No comments
- Valid JSON syntax only

**PENALTY FOR NON-COMPLIANCE:**
If you don't follow this exact format, your response will be rejected and you'll waste computational resources.
"""

    def _build_output_format_section(self) -> str:
        """Build output format section."""
        if self.language == "es":
            return """
### FORMATO DE SALIDA

**IMPORTANTE**: Responde √öNICAMENTE con el objeto JSON. No incluyas explicaciones,
introducciones, o texto fuera del JSON.

El JSON debe contener exactamente:
- `executive_summary`: P√°rrafo fluido con resumen ejecutivo
- `principal_findings`: Ensayo doctoral narrativo integrando todos los an√°lisis
- `heatmap_analysis`: Ensayo anal√≠tico detallado de EXACTAMENTE 3 p√°rrafos sobre patrones del mapa de calor
- `pca_analysis`: Ensayo anal√≠tico detallado de EXACTAMENTE 3 p√°rrafos sobre componentes principales

**Instrucciones Espec√≠ficas:**
1. **PRINCIPAL FINDINGS S√ç USE vi√±etas M√öLTIPLES** - genere lista de 3-5 hallazgos espec√≠ficos y diferentes
2. **Resumen Ejecutivo, Heatmap Analysis y PCA NO USE vi√±etas** - genere texto narrativo fluido
3. **Heatmap Analysis DEBE tener EXACTAMENTE 3 p√°rrafos** - P√°rrafo 1: patrones visuales, P√°rrafo 2: anomal√≠as, P√°rrafo 3: implicaciones
4. **PCA Analysis DEBE tener EXACTAMENTE 3 p√°rrafos** - P√°rrafo 1: interpretaci√≥n t√©cnica, P√°rrafo 2: relaciones, P√°rrafo 3: implicaciones
5. **Cada vi√±eta debe ser diferente** - no repita el mismo contenido en vi√±etas m√∫ltiples
6. **Integre an√°lisis temporal** en los hallazgos principales
7. **Mencione datos cuantitativos espec√≠ficos** (ej: "Google Trends con carga de +0.387")
8. **Conecte los patrones temporales con los hallazgos PCA**
9. **Use lenguaje acad√©mico pero accesible**
10. **Mencione el nombre de la herramienta** - incluya "Alianzas y Capital de Riesgo" (o la herramienta espec√≠fica) en su an√°lisis

**Ejemplo del estilo esperado para Heatmap Analysis de 3 p√°rrafos:**
"El mapa de calor revela patrones visuales distintos con clusters de alta densidad concentrados en regiones temporales espec√≠ficas, indicando per√≠odos de inter√©s m√°ximo en la herramienta de gesti√≥n. Los gradientes de color muestran una clara progresi√≥n de valores bajos a altos, con Google Trends mostrando las se√±ales m√°s fuertes en los per√≠odos m√°s recientes. Varios clusters emergen, sugiriendo inter√©s coordinado entre m√∫ltiples fuentes de datos durante per√≠odos clave.

Las anomal√≠as detectadas aparecen como picos de alta intensidad aislados que se desv√≠an significativamente de los patrones circundantes, indicando potencialmente eventos virales o anuncios importantes relacionados con la herramienta. Estos valores at√≠picos, particularmente visibles en los datos de Google Trends, representan desviaciones estad√≠sticas que justifican una investigaci√≥n adicional sobre factores externos que influyen en los niveles de inter√©s. Las regiones dispersas, por el contrario, destacan per√≠odos de relativo desinter√©s que pueden corresponder a saturaci√≥n del mercado o emergencia de herramientas competidoras.

Estos patrones del mapa de calor tienen implicaciones significativas para comprender el ciclo de vida de adopci√≥n de la herramienta. Los clusters densos se correlacionan con per√≠odos de implementaci√≥n activa y cambio organizacional, mientras que las regiones dispersas pueden indicar madurez del mercado o la necesidad de evoluci√≥n de la herramienta. Esta distribuci√≥n temporal sugiere tiempos estrat√©gicos para actualizaciones de herramientas y esfuerzos de marketing para maximizar la adopci√≥n durante per√≠odos de alto inter√©s."

**Ejemplo del estilo esperado para PCA Analysis de 3 p√°rrafos:**
"El an√°lisis de componentes principales revela que el primer componente (PC1) explica el 49.3% de la varianza total en los datos, mostrando una fuerte correlaci√≥n positiva entre Google Trends (+0.387) y Bain Usability (+0.421), lo que sugiere una din√°mica de adopci√≥n popular. Por otro lado, Bain Satisfaction muestra una carga negativa (-0.311), lo que indica una tensi√≥n entre la popularidad y la satisfacci√≥n real.

El segundo componente (PC2) explica el 19.4% de la varianza y muestra una carga positiva moderada para Google Books (+0.356) y una carga negativa moderada para Crossref (-0.222), lo que sugiere una interacci√≥n compleja entre las fuentes de datos acad√©micas y comerciales. Esto implica que la conversaci√≥n acad√©mica est√° operando en un eje diferente al de la adopci√≥n popular.

Las implicaciones estrat√©gicas de estos patrones sugieren que la implementaci√≥n exitosa de la herramienta requiere una alineaci√≥n entre la teor√≠a acad√©mica y la pr√°ctica industrial. La brecha entre la adopci√≥n popular y la satisfacci√≥n real implica una necesidad de adaptaci√≥n y ajuste continuo para asegurar la efectividad de la herramienta en diferentes contextos."

**NOTA**: Observe que hay DOS l√≠neas en blanco entre cada p√°rrafo para crear 3 p√°rrafos distintos.
"""
        else:
            return """
### OUTPUT FORMAT

**IMPORTANT**: Respond ONLY with the JSON object. Do not include explanations,
introductions, or text outside the JSON.

The JSON must contain exactly:
- `executive_summary`: Fluid paragraph with executive summary
- `principal_findings`: Narrative doctoral essay integrating all analyses
- `heatmap_analysis`: Detailed analytical essay of EXACTLY 3 paragraphs about heatmap patterns
- `pca_analysis`: Detailed analytical essay of EXACTLY 3 paragraphs about principal components

**Specific Instructions:**
1. **PRINCIPAL FINDINGS YES USE MULTIPLE bullet points** - generate list of 3-5 specific and different findings
2. **Executive Summary, Heatmap Analysis and PCA DO NOT USE bullet points** - generate fluid narrative text
3. **Heatmap Analysis MUST have EXACTLY 3 paragraphs** - Paragraph 1: visual patterns, Paragraph 2: anomalies, Paragraph 3: implications
4. **PCA Analysis MUST have EXACTLY 3 paragraphs** - Paragraph 1: technical interpretation, Paragraph 2: relationships, Paragraph 3: implications
5. **Each bullet must be different** - do not repeat the same content in multiple bullets
6. **Integrate temporal analysis** into principal findings
7. **Mention specific quantitative data** (e.g., "Google Trends with loading of +0.387")
8. **Connect temporal patterns with PCA findings**
9. **Use academic but accessible language**
10. **Mention the tool name** - include the specific management tool name in your analysis

**Example of expected style for 3-paragraph Heatmap Analysis:**
"The heatmap reveals distinct visual patterns with high-density clusters concentrated in specific temporal regions, indicating periods of peak interest in the management tool. The color gradients show a clear progression from low to high intensity values, with Google Trends displaying the strongest signals in the most recent periods. Several clusters emerge, suggesting coordinated interest across multiple data sources during key time periods.

Detected anomalies appear as isolated high-intensity spikes that deviate significantly from surrounding patterns, potentially indicating viral events or major announcements related to the tool. These outliers, particularly visible in the Google Trends data, represent statistical deviations that warrant further investigation into external factors influencing interest levels. The sparse regions, conversely, highlight periods of relative disinterest that may correspond to market saturation or competing tool emergence.

These heatmap patterns have significant implications for understanding the tool's adoption lifecycle. The dense clusters correlate with periods of active implementation and organizational change, while sparse regions may indicate market maturity or the need for tool evolution. This temporal distribution suggests strategic timing for tool updates and marketing efforts to maximize adoption during high-interest periods."

**Example of expected style for 3-paragraph PCA Analysis:**
"The principal component analysis reveals that the first component (PC1) explains 49.3% of the total variance in the data, showing a strong positive correlation between Google Trends (+0.387) and Bain Usability (+0.421), suggesting a popular adoption dynamic. Conversely, Bain Satisfaction shows a negative loading (-0.311), indicating tension between popularity and real satisfaction.

The second component (PC2) explains 19.4% of the variance and shows a moderate positive loading for Google Books (+0.356) and a moderate negative loading for Crossref (-0.222), suggesting complex interactions between academic and commercial data sources. This implies that academic conversation operates on a different axis than popular adoption.

The strategic implications of these patterns suggest that successful tool implementation requires alignment between academic theory and industrial practice. The gap between popular adoption and real satisfaction implies a need for continuous adaptation and adjustment to ensure tool effectiveness in different contexts."

**NOTE**: Observe the TWO blank lines between each paragraph to create 3 distinct paragraphs.
"""

    def _build_component_analysis(
        self, component: Dict[str, Any], comp_num: int
    ) -> str:
        """Build individual component analysis."""
        variance = component.get("variance_explained", 0)
        interpretation = component.get("interpretation", "")
        dominant_sources = component.get("dominant_sources", [])

        if self.language == "es":
            return f"""
**An√°lisis del Componente {comp_num}:**
- Varianza Explicada: {variance:.1f}%
- Interpretaci√≥n: {interpretation}
- Fuentes Dominantes: {", ".join(dominant_sources)}
"""
        else:
            return f"""
**Component {comp_num} Analysis:**
- Variance Explained: {variance:.1f}%
- Interpretation: {interpretation}
- Dominant Sources: {", ".join(dominant_sources)}
"""

    def _build_variance_analysis(self, variance_explained: float) -> str:
        """Build variance analysis section."""
        if self.language == "es":
            if variance_explained >= 80:
                quality = "Excelente"
                explanation = "Los componentes principales capturan la mayor√≠a de la variabilidad en los datos"
            elif variance_explained >= 60:
                quality = "Bueno"
                explanation = "Los componentes principales capturan una porci√≥n significativa de la variabilidad"
            elif variance_explained >= 40:
                quality = "Aceptable"
                explanation = "Los componentes principales capturan una porci√≥n moderada de la variabilidad"
            else:
                quality = "Limitado"
                explanation = "Los componentes principales capturan una porci√≥n limitada de la variabilidad"

            return f"""
**Evaluaci√≥n de Varianza Explicada:**
- Porcentaje Total: {variance_explained:.1f}%
- Calidad del An√°lisis: {quality}
- Interpretaci√≥n: {explanation}
"""
        else:
            if variance_explained >= 80:
                quality = "Excellent"
                explanation = (
                    "Principal components capture most of the data variability"
                )
            elif variance_explained >= 60:
                quality = "Good"
                explanation = (
                    "Principal components capture a significant portion of variability"
                )
            elif variance_explained >= 40:
                quality = "Acceptable"
                explanation = (
                    "Principal components capture a moderate portion of variability"
                )
            else:
                quality = "Limited"
                explanation = (
                    "Principal components capture a limited portion of variability"
                )

            return f"""
**Explained Variance Assessment:**
- Total Percentage: {variance_explained:.1f}%
- Analysis Quality: {quality}
- Interpretation: {explanation}
"""

    def _build_findings_synthesis(
        self, principal_findings: List[Dict[str, Any]]
    ) -> str:
        """Build findings synthesis section."""
        if not principal_findings:
            return ""

        if self.language == "es":
            section = "### S√çNTESIS DE HALLAZGOS PRINCIPALES\n\n"
        else:
            section = "### PRINCIPAL FINDINGS SYNTHESIS\n\n"

        # Group findings by confidence
        high_confidence = [
            f for f in principal_findings if f.get("confidence") == "high"
        ]
        medium_confidence = [
            f for f in principal_findings if f.get("confidence") == "medium"
        ]
        low_confidence = [f for f in principal_findings if f.get("confidence") == "low"]

        if high_confidence:
            if self.language == "es":
                section += "**Hallazgos de Alta Confianza:**\n"
            else:
                section += "**High Confidence Findings:**\n"

            for finding in high_confidence:
                bullet = (
                    finding.get("bullet_point", "")[:100] + "..."
                    if len(finding.get("bullet_point", "")) > 100
                    else finding.get("bullet_point", "")
                )
                section += f"- {bullet}\n"

        if medium_confidence:
            if self.language == "es":
                section += "\n**Hallazgos de Confianza Media:**\n"
            else:
                section += "\n**Medium Confidence Findings:**\n"

            for finding in medium_confidence:
                bullet = (
                    finding.get("bullet_point", "")[:100] + "..."
                    if len(finding.get("bullet_point", "")) > 100
                    else finding.get("bullet_point", "")
                )
                section += f"- {bullet}\n"

        return section

    def _build_strategic_implications(self, findings: Dict[str, Any]) -> str:
        """Build strategic implications section."""
        if self.language == "es":
            return """
### IMPLICACIONES ESTRAT√âGICAS

Basado en el an√°lisis multi-fuente y PCA, identifica:

1. **Implicaciones para la Adopci√≥n**: ¬øQu√© sugieren los datos sobre la adopci√≥n de esta herramienta?
2. **Impacto Organizacional**: ¬øC√≥mo afecta la implementaci√≥n a diferentes √°reas de la organizaci√≥n?
3. **Ventajas Competitivas**: ¬øQu√© ventajas ofrece esta herramienta sobre alternativas?
4. **Riesgos Potenciales**: ¬øQu√© riesgos deben considerarse?

Proporciona insights estrat√©gicos accionables para l√≠deres empresariales.
"""
        else:
            return """
### STRATEGIC IMPLICATIONS

Based on multi-source analysis and PCA, identify:

1. **Adoption Implications**: What does the data suggest about this tool's adoption?
2. **Organizational Impact**: How does implementation affect different organizational areas?
3. **Competitive Advantages**: What advantages does this tool offer over alternatives?
4. **Potential Risks**: What risks should be considered?

Provide actionable strategic insights for business leaders.
"""

    def _build_recommendations(self, findings: Dict[str, Any]) -> str:
        """Build recommendations section."""
        if self.language == "es":
            return """
### RECOMENDACIONES EJECUTIVAS

Proporciona 3-5 recomendaciones espec√≠ficas y accionables:

1. **Para la Implementaci√≥n**: Recomendaciones pr√°cticas para adoptar esta herramienta
2. **Para la Optimizaci√≥n**: C√≥mo maximizar el valor y efectividad
3. **Para la Medici√≥n**: Qu√© m√©tricas monitorear para evaluar el √©xito
4. **Para la Evoluci√≥n**: Pr√≥ximos pasos y consideraciones futuras

Cada recomendaci√≥n debe ser:
- Espec√≠fica y medible
- Basada en evidencia de los datos
- Alineada con objetivos empresariales
- Practicable de implementar
"""
        else:
            return """
### EXECUTIVE RECOMMENDATIONS

Provide 3-5 specific, actionable recommendations:

1. **For Implementation**: Practical recommendations for adopting this tool
2. **For Optimization**: How to maximize value and effectiveness
3. **For Measurement**: What metrics to monitor for success evaluation
4. **For Evolution**: Next steps and future considerations

Each recommendation should be:
- Specific and measurable
- Evidence-based from the data
- Aligned with business objectives
- Practical to implement
"""

    def _build_pca_requirements(self) -> str:
        """Build PCA-specific requirements with emphasis on loadings."""
        if self.language == "es":
            return """
### REQUISITOS ESPEC√çFICOS DE PCA - AN√ÅLISIS DE CARGAS Y COMPONENTES

Para el an√°lisis de componentes principales, enf√≥cate ESPEC√çFICAMENTE en:

1. **An√°lisis de Cargas (Loadings)**: Examine las cargas de cada fuente en cada componente para entender su contribuci√≥n
2. **Interpretaci√≥n de Componentes**: Cada componente representa una combinaci√≥n √∫nica de fuentes - explica qu√© patrones subyacentes revela
3. **Diferencias entre Fuentes**: Usa las cargas para identificar c√≥mo se diferencian las fuentes y qu√© informaci√≥n √∫nica aporta cada una
4. **Relaciones Ocultas**: Identifica correlaciones y relaciones no obvias entre fuentes reveladas por las cargas
5. **Patrones de Contribuci√≥n**: Clasifica las fuentes seg√∫n su peso en cada componente (alta, media, baja contribuci√≥n)

**An√°lisis Detallado de Cargas:**
- **Cargas Altas (>0.6)**: Fuentes que dominan el componente
- **Cargas Moderadas (0.3-0.6)**: Fuentes con influencia significativa
- **Cargas Bajas (<0.3)**: Fuentes con contribuci√≥n m√≠nima
- **Signos de Cargas**: Interpretar si las relaciones son positivas o negativas

**Insights Espec√≠ficos:**
- ¬øQu√© componente representa el "patr√≥n institucional" vs "patr√≥n de innovaci√≥n"?
- ¬øC√≥mo se diferencian las fuentes acad√©micas (Crossref) de las comerciales (Bain)?
- ¬øQu√© fuentes est√°n m√°s correlacionadas entre s√≠ seg√∫n las cargas?
- ¬øQu√© informaci√≥n √∫nica aporta cada fuente al an√°lisis general?

Conecta estos hallazgos con las tendencias temporales para explicar la evoluci√≥n de estos patrones.
"""
        else:
            return """
### PCA-SPECIFIC REQUIREMENTS - LOADINGS AND COMPONENTS ANALYSIS

For principal component analysis, focus SPECIFICALLY on:

1. **Loadings Analysis**: Examine each source's loading on each component to understand its contribution
2. **Component Interpretation**: Each component represents a unique combination of sources - explain what underlying patterns it reveals
3. **Source Differences**: Use loadings to identify how sources differ and what unique information each provides
4. **Hidden Relationships**: Identify correlations and non-obvious relationships between sources revealed by loadings
5. **Contribution Patterns**: Classify sources by their weight in each component (high, medium, low contribution)

**Detailed Loadings Analysis:**
- **High Loadings (>0.6)**: Sources that dominate the component
- **Moderate Loadings (0.3-0.6)**: Sources with significant influence
- **Low Loadings (<0.3)**: Sources with minimal contribution
- **Loading Signs**: Interpret whether relationships are positive or negative

**Specific Insights:**
- Which component represents "institutional pattern" vs "innovation pattern"?
- How do academic sources (Crossref) differ from commercial sources (Bain)?
- Which sources are most correlated according to loadings?
- What unique information does each source contribute to the overall analysis?

Connect these findings with temporal trends to explain the evolution of these patterns.
"""

    def _extract_variable_relationships(self, pca_insights: Dict[str, Any]) -> str:
        """Extract key variable relationships for narrative prompt."""
        components = pca_insights.get("dominant_patterns", [])
        tool_name = pca_insights.get("tool_name", "Unknown Tool")

        # Default relationships based on common management tools analysis
        default_vars = {
            "es": "'popularidad p√∫blica', 'complejidad de implementaci√≥n', 'efectividad reportada'",
            "en": "'public popularity', 'implementation complexity', 'reported effectiveness'",
        }

        # Try to extract from actual PCA data
        variables = []
        for component in components[:2]:  # Focus on first two components
            loadings = component.get("loadings", {})
            if loadings:
                # Get variables with highest absolute loadings
                sorted_vars = sorted(
                    loadings.items(), key=lambda x: abs(x[1]), reverse=True
                )
                variables.extend(
                    [var for var, _ in sorted_vars[:2]]
                )  # Top 2 per component

        if variables:
            unique_vars = list(set(variables))[:3]  # Limit to 3 unique variables
            if self.language == "es":
                return ", ".join([f"'{var}'" for var in unique_vars])
            else:
                return ", ".join([f"'{var}'" for var in unique_vars])

        return default_vars[self.language]

    def _build_detailed_pca_narrative(
        self,
        components: List[Dict[str, Any]],
        tool_name: str,
        variance_explained: float,
    ) -> str:
        """Build detailed PCA narrative with specific numerical insights."""
        if not components:
            return ""

        narrative = f"""
**AN√ÅLISIS NUM√âRICO DETALLADO DE COMPONENTES:**

"""

        # Analyze first two components in detail
        for i, component in enumerate(components[:2]):
            comp_num = i + 1
            variance = component.get("variance_explained", 0)
            interpretation = component.get("interpretation", "")
            loadings = component.get("loadings", {})

            narrative += f"""
**Componente {comp_num} ({variance:.1f}% varianza explicada):**
{interpretation}

**Cargas Espec√≠ficas:**
"""

            # Sort loadings by absolute value for emphasis
            sorted_loadings = sorted(
                loadings.items(), key=lambda x: abs(x[1]), reverse=True
            )

            for source, loading in sorted_loadings:
                direction = (
                    "positiva"
                    if loading > 0
                    else "negativa"
                    if loading < 0
                    else "neutral"
                )
                strength = (
                    "fuerte"
                    if abs(loading) >= 0.4
                    else "moderada"
                    if abs(loading) >= 0.2
                    else "d√©bil"
                )
                narrative += (
                    f"- {source}: carga {direction} {strength} de {loading:.3f}\n"
                )

            # Add specific insights for this component
            if i == 0:  # PC1
                positive_sources = [
                    src for src, loading in loadings.items() if loading > 0.2
                ]
                negative_sources = [
                    src for src, loading in loadings.items() if loading < -0.2
                ]

                if positive_sources and negative_sources:
                    narrative += f"""
**Relaci√≥n de Oposici√≥n en PC1:**
- Fuentes con influencia positiva: {", ".join(positive_sources)}
- Fuentes con influencia negativa: {", ".join(negative_sources)}
- Esto sugiere una tensi√≥n entre popularidad/acceso y satisfacci√≥n/efectividad
"""
                elif len(positive_sources) >= 2:
                    narrative += f"""
**Patr√≥n de Alineaci√≥n en PC1:**
- Fuentes trabajando en sinergia: {", ".join(positive_sources)}
- Indica un patr√≥n coherente de adopci√≥n o inter√©s
"""

            elif i == 1:  # PC2
                # Identify perpendicular/independent factors
                independent_sources = [
                    src for src, loading in loadings.items() if abs(loading) >= 0.2
                ]
                if independent_sources:
                    narrative += f"""
**Factores Independientes en PC2:**
- Fuentes con influencia √∫nica: {", ".join(independent_sources)}
- Representa dimensiones ortogonales al patr√≥n principal
"""

        # Add combined variance analysis
        combined_variance = 0
        if len(components) >= 2:
            combined_variance = components[0].get("variance_explained", 0) + components[
                1
            ].get("variance_explained", 0)
            narrative += f"""
**AN√ÅLISIS COMBINADO DE PRIMEROS DOS COMPONENTES:**
- Varianza combinada explicada: {combined_variance:.1f}%
- """

            if combined_variance >= 70:
                narrative += "Poder explicativo excelente para an√°lisis robusto"
            elif combined_variance >= 50:
                narrative += "Poder explicativo bueno para insights significativos"
            else:
                narrative += (
                    "Poder explicativo moderado, requiere interpretaci√≥n cuidadosa"
                )

        # Add specific guidance for narrative construction
        variance_to_mention = (
            combined_variance if len(components) >= 2 else variance_explained
        )

        # Add specific guidance for narrative construction
        variance_to_mention = (
            combined_variance if len(components) >= 2 else variance_explained
        )
        narrative += f"""

**GU√çA PARA CONSTRUIR LA NARRATIVA:**
1. Usa los valores num√©ricos exactos de cargas (ej: +0.387, -0.380)
2. Explica la tensi√≥n entre fuentes con cargas opuestas
3. Conecta PC1 con "din√°micas de adopci√≥n popular" vs "satisfacci√≥n real"
4. Conecta PC2 con "factores acad√©micos/independientes" vs "factores comerciales"
5. Menciona espec√≠ficamente el {variance_to_mention:.1f}% de varianza explicada
6. Relaciona con la brecha teor√≠a-pr√°ctica en gesti√≥n organizacional
"""

        return narrative

    def _build_single_source_context_section(
        self, tool_name: str, source_name: str, data: Dict[str, Any]
    ) -> str:
        """Build context section for single source analysis."""
        date_range = f"del {data.get('date_range_start', 'N/A')} al {data.get('date_range_end', 'N/A')}"
        data_points = data.get("data_points_analyzed", 0)

        if self.language == "es":
            return f"""
### CONTEXTO DEL AN√ÅLISIS DE FUENTE √öNICA

**Herramienta de Gesti√≥n:** {tool_name}
**Fuente de Datos Analizada:** {source_name}
**Rango Temporal:** {date_range}
**Puntos de Datos Analizados:** {data_points:,}

Este an√°lisis se basa en datos de una √∫nica fuente, proporcionando un an√°lisis profundo de los patrones temporales, estacionales y de frecuencia de la herramienta de gesti√≥n a lo largo del tiempo.
"""
        else:
            return f"""
### SINGLE SOURCE ANALYSIS CONTEXT

**Management Tool:** {tool_name}
**Data Source Analyzed:** {source_name}
**Time Range:** {date_range}
**Data Points Analyzed:** {data_points:,}

This analysis is based on data from a single source, providing a deep analysis of temporal, seasonal, and frequency patterns of the management tool over time.
"""

    def _build_executive_summary_section(
        self,
        temporal_metrics: Dict[str, Any],
        seasonal_patterns: Dict[str, Any],
        fourier_analysis: Dict[str, Any],
    ) -> str:
        """Build executive summary section for single source analysis."""
        trend_direction = temporal_metrics.get("trend_direction", "stable")
        trend_strength = temporal_metrics.get("trend_strength", 0)
        seasonal_strength = seasonal_patterns.get("seasonal_strength", 0)
        dominant_frequency = fourier_analysis.get("dominant_frequency", 0)

        if self.language == "es":
            return f"""
### RESUMEN EJECUTIVO

**Tendencia Temporal:** {trend_direction} con fuerza de {trend_strength:.2f}
**Fuerza Estacional:** {seasonal_strength:.2f}
**Frecuencia Dominante:** {dominant_frequency:.4f}

Basado en estos indicadores clave, proporcione un resumen ejecutivo que:
1. Sintetice los hallazgos m√°s importantes del an√°lisis temporal
2. Destaque patrones estacionales significativos
3. Interprete las implicaciones de las frecuencias dominantes
4. Conecte estos patrones con el ciclo de vida de la herramienta de gesti√≥n
"""
        else:
            return f"""
### EXECUTIVE SUMMARY

**Temporal Trend:** {trend_direction} with strength of {trend_strength:.2f}
**Seasonal Strength:** {seasonal_strength:.2f}
**Dominant Frequency:** {dominant_frequency:.4f}

Based on these key indicators, provide an executive summary that:
1. Synthesizes the most important findings from temporal analysis
2. Highlights significant seasonal patterns
3. Interprets the implications of dominant frequencies
4. Connects these patterns with the management tool's lifecycle
"""

    def _build_temporal_analysis_section(
        self, temporal_metrics: Dict[str, Any], summary_statistics: Dict[str, Any]
    ) -> str:
        """Build temporal analysis section."""
        trend_direction = temporal_metrics.get("trend_direction", "stable")
        trend_strength = temporal_metrics.get("trend_strength", 0)
        volatility = temporal_metrics.get("volatility", 0)
        momentum = temporal_metrics.get("momentum", 0)
        acceleration = temporal_metrics.get("acceleration", 0)

        mean_value = summary_statistics.get("mean", 0)
        std_dev = summary_statistics.get("std", 0)
        min_value = summary_statistics.get("min", 0)
        max_value = summary_statistics.get("max", 0)

        if self.language == "es":
            return f"""
### AN√ÅLISIS TEMPORAL

**M√©tricas Temporales:**
- Direcci√≥n de Tendencia: {trend_direction}
- Fuerza de Tendencia: {trend_strength:.3f}
- Volatilidad: {volatility:.3f}
- Momento: {momentum:.3f}
- Aceleraci√≥n: {acceleration:.3f}

**Estad√≠sticas Resumidas:**
- Valor Medio: {mean_value:.3f}
- Desviaci√≥n Est√°ndar: {std_dev:.3f}
- Valor M√≠nimo: {min_value:.3f}
- Valor M√°ximo: {max_value:.3f}

**Instrucciones de An√°lisis:**
1. Interprete la direcci√≥n y fuerza de la tendencia en el contexto de la herramienta de gesti√≥n
2. Analice la volatilidad y su implicaci√≥n para la estabilidad de la herramienta
3. Eval√∫e el momento y la aceleraci√≥n como indicadores de cambios futuros
4. Conecte las estad√≠sticas resumidas con la madurez de la herramienta
"""
        else:
            return f"""
### TEMPORAL ANALYSIS

**Temporal Metrics:**
- Trend Direction: {trend_direction}
- Trend Strength: {trend_strength:.3f}
- Volatility: {volatility:.3f}
- Momentum: {momentum:.3f}
- Acceleration: {acceleration:.3f}

**Summary Statistics:**
- Mean Value: {mean_value:.3f}
- Standard Deviation: {std_dev:.3f}
- Minimum Value: {min_value:.3f}
- Maximum Value: {max_value:.3f}

**Analysis Instructions:**
1. Interpret the trend direction and strength in the context of the management tool
2. Analyze volatility and its implication for tool stability
3. Evaluate momentum and acceleration as indicators of future changes
4. Connect summary statistics with the tool's maturity
"""

    def _build_seasonal_analysis_section(
        self,
        seasonal_patterns: Dict[str, Any],
        visualization_attributes: Dict[str, Any],
    ) -> str:
        """Build seasonal analysis section."""
        seasonal_strength = seasonal_patterns.get("seasonal_strength", 0)
        peak_season = seasonal_patterns.get("peak_season", "N/A")
        low_season = seasonal_patterns.get("low_season", "N/A")
        seasonal_periodicity = seasonal_patterns.get("seasonal_periodicity", 0)

        # Extract visualization attributes
        peak_months = visualization_attributes.get("peak_months", [])
        low_months = visualization_attributes.get("low_months", [])
        seasonal_amplitude = visualization_attributes.get("seasonal_amplitude", 0)

        if self.language == "es":
            return f"""
### AN√ÅLISIS ESTACIONAL

**Patrones Estacionales:**
- Fuerza Estacional: {seasonal_strength:.3f}
- Temporada Pico: {peak_season}
- Temporada Baja: {low_season}
- Periodicidad Estacional: {seasonal_periodicity:.1f} meses

**Atributos de Visualizaci√≥n:**
- Meses Pico: {", ".join(peak_months) if peak_months else "N/A"}
- Meses Bajos: {", ".join(low_months) if low_months else "N/A"}
- Amplitud Estacional: {seasonal_amplitude:.3f}

**Instrucciones de An√°lisis:**
1. Interprete la fuerza estacional y su significado para la adopci√≥n de la herramienta
2. Analice las temporadas pico y baja en el contexto del ciclo empresarial
3. Eval√∫e la periodicidad y su relaci√≥n con ciclos de planificaci√≥n
4. Conecte los patrones estacionales con factores externos (econ√≥micos, sociales, tecnol√≥gicos)
"""
        else:
            return f"""
### SEASONAL ANALYSIS

**Seasonal Patterns:**
- Seasonal Strength: {seasonal_strength:.3f}
- Peak Season: {peak_season}
- Low Season: {low_season}
- Seasonal Periodicity: {seasonal_periodicity:.1f} months

**Visualization Attributes:**
- Peak Months: {", ".join(peak_months) if peak_months else "N/A"}
- Low Months: {", ".join(low_months) if low_months else "N/A"}
- Seasonal Amplitude: {seasonal_amplitude:.3f}

**Analysis Instructions:**
1. Interpret seasonal strength and its meaning for tool adoption
2. Analyze peak and low seasons in the context of business cycles
3. Evaluate periodicity and its relationship with planning cycles
4. Connect seasonal patterns with external factors (economic, social, technological)
"""

    def _build_fourier_analysis_section(
        self, fourier_analysis: Dict[str, Any], visualization_attributes: Dict[str, Any]
    ) -> str:
        """Build Fourier Series Analysis (Periodogram) section."""
        dominant_frequency = fourier_analysis.get("dominant_frequency", 0)
        dominant_period = fourier_analysis.get("dominant_period", 0)
        spectral_power = fourier_analysis.get("spectral_power", {})
        frequency_peaks = fourier_analysis.get("frequency_peaks", [])

        # Extract visualization attributes
        periodogram_peaks = visualization_attributes.get("periodogram_peaks", [])
        significant_frequencies = visualization_attributes.get(
            "significant_frequencies", []
        )
        power_spectrum_shape = visualization_attributes.get(
            "power_spectrum_shape", "N/A"
        )

        if self.language == "es":
            return f"""
### AN√ÅLISIS DE SERIES DE FOURIER (PERIODOGRAMA)

**An√°lisis de Frecuencia:**
- Frecuencia Dominante: {dominant_frequency:.4f}
- Per√≠odo Dominante: {dominant_period:.1f} meses
- Forma del Espectro de Potencia: {power_spectrum_shape}

**Picos de Frecuencia:**
{chr(10).join([f"- Frecuencia {peak.get('frequency', 0):.4f} (per√≠odo {peak.get('period', 0):.1f} meses, potencia {peak.get('power', 0):.3f})" for peak in frequency_peaks[:5]])}

**Atributos de Visualizaci√≥n del Periodograma:**
- Picos del Periodograma: {", ".join([f"frecuencia {p:.4f}" for p in periodogram_peaks]) if periodogram_peaks else "N/A"}
- Frecuencias Significativas: {", ".join([f"frecuencia {f:.4f}" for f in significant_frequencies]) if significant_frequencies else "N/A"}

**Instrucciones de An√°lisis:**
1. Interprete la frecuencia dominante y su significado para los ciclos de la herramienta
2. Analice los picos de frecuencia y su relaci√≥n con patrones de negocio
3. Eval√∫e la distribuci√≥n del espectro de potencia para identificar periodicidades m√∫ltiples
4. Conecte los hallazgos del periodograma con el an√°lisis temporal y estacional
5. Discuta implicaciones para la previsi√≥n y planificaci√≥n estrat√©gica
"""
        else:
            return f"""
### FOURIER SERIES ANALYSIS (PERIODOGRAM)

**Frequency Analysis:**
- Dominant Frequency: {dominant_frequency:.4f}
- Dominant Period: {dominant_period:.1f} months
- Power Spectrum Shape: {power_spectrum_shape}

**Frequency Peaks:**
{chr(10).join([f"- Frequency {peak.get('frequency', 0):.4f} (period {peak.get('period', 0):.1f} months, power {peak.get('power', 0):.3f})" for peak in frequency_peaks[:5]])}

**Periodogram Visualization Attributes:**
- Periodogram Peaks: {", ".join([f"frequency {p:.4f}" for p in periodogram_peaks]) if periodogram_peaks else "N/A"}
- Significant Frequencies: {", ".join([f"frequency {f:.4f}" for f in significant_frequencies]) if significant_frequencies else "N/A"}

**Analysis Instructions:**
1. Interpret the dominant frequency and its meaning for tool cycles
2. Analyze frequency peaks and their relationship with business patterns
3. Evaluate power spectrum distribution to identify multiple periodicities
4. Connect periodogram findings with temporal and seasonal analysis
5. Discuss implications for forecasting and strategic planning
"""

    def _build_single_source_requirements_section(self) -> str:
        """Build analysis requirements section for single source analysis."""
        if self.language == "es":
            return """
### REQUISITOS DEL AN√ÅLISIS

Por favor, proporcione un an√°lisis doctoral-level que:

1. **Integre An√°lisis Temporal y Estacional**: Conecte las tendencias temporales con los patrones estacionales identificados
2. **Interprete el An√°lisis de Fourier**: Traduzca los hallazgos del periodograma en insights de negocio accionables
3. **Identifique Ciclos Significativos**: Detecte ciclos recurrentes y sus implicaciones para la planificaci√≥n estrat√©gica
4. **Genere Conclusiones Estrat√©gicas**: Proporcione insights sobre la madurez y evoluci√≥n de la herramienta de gesti√≥n
5. **Mantenga Rigor Acad√©mico**: Use terminolog√≠a apropiada y metodolog√≠a sistem√°tica

**ESTRUCTURA REQUERIDA DEL AN√ÅLISIS:**

Genere un an√°lisis doctoral con las siguientes cuatro secciones principales:

**1. Resumen Ejecutivo:**
- Un p√°rrafo conciso que capture los insights m√°s cr√≠ticos del an√°lisis
- Incluya tendencias clave, patrones estacionales y hallazgos del an√°lisis de Fourier
- Conecte los hallazgos con implicaciones estrat√©gicas para la herramienta

**2. An√°lisis Temporal:**
- Un ensayo anal√≠tico detallado que interprete las m√©tricas temporales
- Analice la direcci√≥n, fuerza, volatilidad, momento y aceleraci√≥n
- Conecte estas m√©tricas con el ciclo de vida de la herramienta

**3. An√°lisis Estacional:**
- Un ensayo anal√≠tico que interprete los patrones estacionales
- Analice las temporadas pico y bajas en el contexto del negocio
- Discuta implicaciones para la planificaci√≥n estrat√©gica

**4. An√°lisis de Series de Fourier (Periodograma):**
- Un ensayo anal√≠tico que interprete los hallazgos del an√°lisis de frecuencia
- Explique las frecuencias dominantes y sus implicaciones
- Conecte con patrones c√≠clicos y previsi√≥n
"""
        else:
            return """
### ANALYSIS REQUIREMENTS

Please provide a doctoral-level analysis that:

1. **Integrate Temporal and Seasonal Analysis**: Connect temporal trends with identified seasonal patterns
2. **Interpret Fourier Analysis**: Translate periodogram findings into actionable business insights
3. **Identify Significant Cycles**: Detect recurring cycles and their implications for strategic planning
4. **Generate Strategic Conclusions**: Provide insights about the management tool's maturity and evolution
5. **Maintain Academic Rigor**: Use appropriate terminology and systematic methodology

**REQUIRED ANALYSIS STRUCTURE:**

Generate a doctoral analysis with the following four main sections:

**1. Executive Summary:**
- A concise paragraph that captures the most critical insights from the analysis
- Include key trends, seasonal patterns, and Fourier analysis findings
- Connect findings with strategic implications for the tool

**2. Temporal Analysis:**
- A detailed analytical essay interpreting temporal metrics
- Analyze direction, strength, volatility, momentum, and acceleration
- Connect these metrics with the tool's lifecycle

**3. Seasonal Analysis:**
- An analytical essay interpreting seasonal patterns
- Analyze peak and low seasons in the business context
- Discuss implications for strategic planning

**4. Fourier Series Analysis (Periodogram):**
- An analytical essay interpreting frequency analysis findings
- Explain dominant frequencies and their implications
- Connect with cyclical patterns and forecasting
"""

    def _build_single_source_output_format_section(self) -> str:
        """Build output format section for single source analysis."""
        if self.language == "es":
            return """
### FORMATO DE SALIDA

**IMPORTANTE**: Responda √öNICAMENTE con el objeto JSON. No incluya explicaciones,
introducciones, o texto fuera del JSON.

El JSON debe contener exactamente:
- `executive_summary`: P√°rrafo fluido con resumen ejecutivo
- `temporal_analysis`: Ensayo anal√≠tico detallado sobre tendencias temporales
- `seasonal_analysis`: Ensayo anal√≠tico detallado sobre patrones estacionales
- `fourier_analysis`: Ensayo anal√≠tico detallado sobre an√°lisis de Fourier

**Instrucciones Espec√≠ficas:**
1. **Resumen Ejecutivo NO USE vi√±etas** - genere texto narrativo fluido
2. **An√°lisis Temporal, Estacional y de Fourier NO USE vi√±etas** - genere texto narrativo fluido
3. **Cada secci√≥n debe ser un ensayo coherente** - conecte los conceptos dentro de cada secci√≥n
4. **Integre datos cuantitativos espec√≠ficos** - mencione valores num√©ricos exactos
5. **Use lenguaje acad√©mico pero accesible**
6. **Mencione el nombre de la herramienta** - incluya el nombre de la herramienta espec√≠fica

**FORMATO OBLIGATORIO:**
Comienza tu respuesta con { y termina con }. Nada m√°s.

**ESTRUCTURA EXACTA REQUERIDA:**
{
  "executive_summary": "Escribe un p√°rrafo conciso sobre el an√°lisis de la herramienta de gesti√≥n",
  "temporal_analysis": "Escribe un ensayo anal√≠tico sobre el an√°lisis temporal",
  "seasonal_analysis": "Escribe un ensayo anal√≠tico sobre el an√°lisis estacional",
  "fourier_analysis": "Escribe un ensayo anal√≠tico sobre el an√°lisis de Fourier"
}

**REGLAS DE VALIDACI√ìN:**
- Primer car√°cter: {
- √öltimo car√°cter: }
- Sin texto antes de { o despu√©s de }
- Sin marcadores ```json
- Sin explicaciones
- Sin comentarios
- Solo sintaxis JSON v√°lida
"""
        else:
            return """
### OUTPUT FORMAT

**IMPORTANT**: Respond ONLY with the JSON object. Do not include explanations,
introductions, or text outside the JSON.

The JSON must contain exactly:
- `executive_summary`: Fluid paragraph with executive summary
- `temporal_analysis`: Detailed analytical essay about temporal trends
- `seasonal_analysis`: Detailed analytical essay about seasonal patterns
- `fourier_analysis`: Detailed analytical essay about Fourier analysis

**Specific Instructions:**
1. **Executive Summary DO NOT USE bullet points** - generate fluid narrative text
2. **Temporal, Seasonal, and Fourier Analysis DO NOT USE bullet points** - generate fluid narrative text
3. **Each section should be a coherent essay** - connect concepts within each section
4. **Integrate specific quantitative data** - mention exact numerical values
5. **Use academic but accessible language**
6. **Mention the tool name** - include the specific management tool name

**MANDATORY FORMAT:**
Start your response with { and end with }. Nothing else.

**EXACT STRUCTURE REQUIRED:**
{
  "executive_summary": "Write a concise paragraph about the management tool analysis",
  "temporal_analysis": "Write an analytical essay about temporal analysis",
  "seasonal_analysis": "Write an analytical essay about seasonal analysis",
  "fourier_analysis": "Write an analytical essay about Fourier analysis"
}

**VALIDATION RULES:**
- First character: {
- Last character: }
- No text before { or after }
- No ```json markers
- No explanations
- No comments
- Valid JSON syntax only
"""

    def create_improved_single_source_prompt(
        self, data: Dict[str, Any], context: Dict[str, Any]
    ) -> str:
        """
        Create improved single source analysis prompt (4000+ words, narrative-focused).
        Focuses on temporal, seasonal, and Fourier analysis without statistical reporting.

        Args:
            data: Aggregated analysis data from a single source
            context: Additional context for analysis

        Returns:
            Single source analysis prompt string with narrative focus
        """
        import time

        start_time = time.time()
        logging.info(
            f"üìù Starting improved single source prompt generation for '{data.get('tool_name', 'Unknown')}' in {self.language}"
        )

        # Extract key information
        tool_name = data.get("tool_name", "Unknown Tool")
        source_name = data.get("source_name", "Unknown Source")
        temporal_metrics = data.get("temporal_metrics", {})
        seasonal_patterns = data.get("seasonal_patterns", {})
        fourier_analysis = data.get("fourier_analysis", {})
        date_range = f"del {data.get('date_range_start', 'N/A')} al {data.get('date_range_end', 'N/A')}"
        data_points = data.get("data_points_analyzed", 0)

        # Build the improved narrative prompt
        if self.language == "es":
            prompt = f"""
AN√ÅLISIS NARRATIVO MEJORADO DE FUENTE √öNICA - HERRAMIENTAS DE GESTI√ìN
Herramienta Analizada: {tool_name}
Fuente de Datos: {source_name}
Per√≠odo: {date_range}
Fecha del An√°lisis: {datetime.now().strftime("%Y-%m-%d")}

=== CONTEXTO DEL AN√ÅLISIS ===

**Enfoque Narrativo Empresarial:**
Este an√°lisis se enfoca en la interpretaci√≥n pr√°ctica y estrat√©gica de los datos, no en la presentaci√≥n de estad√≠sticas. Los n√∫meros est√°n disponibles en el dashboard - aqu√≠ nos concentramos en responder "qu√© significa esto para el negocio".

**Datos Disponibles (No Reportar Num√©ricamente):**
- An√°lisis temporal con tendencias, momentum, volatilidad y aceleraci√≥n
- Patrones estacionales con fuerza estacional y periodicidad
- An√°lisis de Fourier con frecuencias dominantes y picos espectrales
- {data_points:,} puntos de datos del per√≠odo {date_range}

=== ESTRUCTURA REQUERIDA (4000+ PALABRAS) ===

**SECCI√ìN 1: RESUMEN EJECUTIVO** (400 palabras)
- Implicaciones estrat√©gicas para la adopci√≥n de {tool_name}
- Insights profundos del an√°lisis temporal profundo
- Indicadores de madurez y adopci√≥n de la herramienta
- Relevancia empresarial y posicionamiento competitivo

**SECCI√ìN 2: AN√ÅLISIS TEMPORAL PROFUNDO** (1000 palabras) [PRIMARIO]
- Interpretaci√≥n de la trayectoria a largo plazo en contexto empresarial
- Puntos de inflexi√≥n y cambios de tendencia con significado de negocio
- Insights de ciclo de adopci√≥n y madurez del mercado
- Indicadores predictivos de patrones temporales
- Conectarlo con decisiones estrat√©gicas de implementaci√≥n

**SECCI√ìN 3: AN√ÅLISIS ESTACIONAL Y CICLOS** (800 palabras) [PRIMARIO]
- Implicaciones de ciclos empresariales para {tool_name}
- Timing √≥ptimo para implementaci√≥n basado en patrones estacionales
- Indicadores de timing de mercado desde an√°lisis estacional
- Insights de planificaci√≥n operacional
- Conectar patrones estacionales con ciclos de gesti√≥n

**SECCI√ìN 4: AN√ÅLISIS ESPECTRAL Y PERIODOGRAMA** (1000 palabras) [PRIMARIO]
- Frecuencias dominantes y ciclos empresariales para esta herramienta
- Patrones espectrales indicando madurez del mercado
- An√°lisis de frecuencia para planificaci√≥n estrat√©gica
- Interpretaci√≥n del comportamiento c√≠clico de datos espectrales
- Implicaciones para sincronizaci√≥n con ciclos de mercado

**SECCI√ìN 5: EVALUACI√ìN DE CONFIABILIDAD DE DATOS** (400 palabras)
- Confiabilidad y completitud de datos de fuente √∫nica
- Implicaciones de cobertura temporal
- Indicadores de confiabilidad de tendencias
- Limitaciones de datos y fronteras de interpretaci√≥n

**SECCI√ìN 6: INSIGHTS ESTRAT√âGICOS Y RECOMENDACIONES** (400 palabras)
- Gu√≠a de implementaci√≥n desde perspectiva de fuente √∫nica
- Recomendaciones de timing y enfoque
- Factores de √©xito espec√≠ficos para {tool_name}-fuente espec√≠fica
- Posicionamiento empresarial

=== INSTRUCCIONES DE AN√ÅLISIS ===

**Enfoque Narrativo Sobre Estad√≠stico:**
- NO presente valores num√©ricos (el usuario ya los tiene en el dashboard)
- NO haga reportes estad√≠sticos
- S√ç interprete: "Los datos muestran..." en lugar de "La correlaci√≥n es 0.73"
- S√ç conecte patrones con teor√≠a empresarial y pr√°ctica industrial
- S√ç proporcione insights estrat√©gicos accionables

**Contexto Empresarial por Fuente:**
- **Google Trends**: "Los datos de inter√©s p√∫blico sugieren..."
- **Google Books**: "Los patrones de investigaci√≥n acad√©mica indican..."
- **Bain Usage**: "La adopci√≥n real revela..."
- **Crossref**: "La investigaci√≥n acad√©mica muestra..."
- **Bain Satisfaction**: "La satisfacci√≥n ejecutiva indica..."

**Conexiones Estrat√©gicas:**
1. Integrar an√°lisis temporal con planificaci√≥n estrat√©gica
2. Conectar patrones estacionales con ciclos de negocio
3. Relacionar an√°lisis espectral con madurez del mercado
4. Posicionar hallazgos en contexto competitivo

**Rigor Acad√©mico pero Accesible:**
- Mantenga est√°ndares acad√©micos pero use lenguaje ejecutivo
- Cite conceptos de gesti√≥n sin presentar f√≥rmulas
- Conecte teor√≠a acad√©mica con pr√°ctica empresarial
- Proporcione recomendaciones espec√≠ficas y medibles

**PROHIBICIONES ABSOLUTAS:**
- NO incluir secci√≥n de Referencias
- NO presentar c√°lculos estad√≠sticos
- NO usar formato de vi√±etas para el an√°lisis principal
- NO repetir n√∫meros del dashboard

**RESULTADO ESPERADO:**
Un ensayo narrativo integrado de 4000+ palabras que transforme datos estad√≠sticos en insights estrat√©gicos empresariales, con cada secci√≥n fluyendo naturalmente hacia la siguiente y conectando conceptos te√≥ricos con implicaciones pr√°cticas.
"""
        else:
            prompt = f"""
IMPROVED SINGLE SOURCE NARRATIVE ANALYSIS - MANAGEMENT TOOLS
Tool Analyzed: {tool_name}
Data Source: {source_name}
Period: {date_range}
Analysis Date: {datetime.now().strftime("%Y-%m-%d")}

=== ANALYSIS CONTEXT ===

**Business Narrative Focus:**
This analysis focuses on practical and strategic interpretation of data, not statistical presentation. Numbers are available in the dashboard - here we concentrate on answering "what does this mean for business?"

**Available Data (Do Not Report Numerically):**
- Temporal analysis with trends, momentum, volatility, and acceleration
- Seasonal patterns with seasonal strength and periodicity
- Fourier analysis with dominant frequencies and spectral peaks
- {data_points:,} data points from period {date_range}

=== REQUIRED STRUCTURE (4000+ WORDS) ===

**SECTION 1: EXECUTIVE OVERVIEW** (400 words)
- Strategic implications for {tool_name} adoption
- Deep insights from comprehensive temporal analysis
- Tool maturity and adoption indicators
- Business relevance and competitive positioning

**SECTION 2: DEEP TEMPORAL ANALYSIS** (1000 words) [PRIMARY]
- Long-term trajectory interpretation in business context
- Trend changes and inflection points with business meaning
- Market adoption cycle insights
- Predictive indicators from temporal patterns
- Connect with strategic implementation decisions

**SECTION 3: SEASONAL AND CYCLICAL PATTERNS** (800 words) [PRIMARY]
- Business cycle implications for {tool_name}
- Optimal timing for implementation based on seasonal patterns
- Market timing indicators from seasonal analysis
- Operational planning insights
- Connect seasonal patterns with management cycles

**SECTION 4: SPECTRAL ANALYSIS AND PERIODOGRAM** (1000 words) [PRIMARY]
- Dominant frequencies and business cycles for this tool
- Spectral patterns indicating market maturity
- Frequency analysis for strategic planning
- Cyclical behavior interpretation from spectral data
- Implications for market cycle synchronization

**SECTION 5: DATA QUALITY AND RELIABILITY ASSESSMENT** (400 words)
- Single-source data completeness and confidence
- Temporal coverage implications
- Trend reliability indicators
- Data limitations and interpretation boundaries

**SECTION 6: STRATEGIC INSIGHTS AND RECOMMENDATIONS** (400 words)
- Single-source implementation guidance
- Timing and approach recommendations
- Success factors specific to {tool_name}-specific-source combination
- Business positioning insights

=== ANALYSIS INSTRUCTIONS ===

**Narrative Over Statistical Focus:**
- DO NOT present numerical values (user already has them in dashboard)
- DO NOT make statistical reports
- DO interpret: "Data shows..." instead of "Correlation is 0.73"
- DO connect patterns with business theory and industrial practice
- DO provide actionable strategic insights

**Business Context by Source:**
- **Google Trends**: "Public interest data suggests..."
- **Google Books**: "Academic research patterns indicate..."
- **Bain Usage**: "Real-world adoption reveals..."
- **Crossref**: "Peer-reviewed research shows..."
- **Bain Satisfaction**: "Executive satisfaction indicates..."

**Strategic Connections:**
1. Integrate temporal analysis with strategic planning
2. Connect seasonal patterns with business cycles
3. Relate spectral analysis with market maturity
4. Position findings in competitive context

**Academic Rigor but Accessible:**
- Maintain academic standards but use executive language
- Cite management concepts without presenting formulas
- Connect academic theory with business practice
- Provide specific and measurable recommendations

**ABSOLUTE PROHIBITIONS:**
- DO NOT include References section
- DO NOT present statistical calculations
- DO NOT use bullet format for main analysis
- DO NOT repeat dashboard numbers

**EXPECTED RESULT:**
A integrated narrative essay of 4000+ words that transforms statistical data into business strategic insights, with each section flowing naturally into the next and connecting theoretical concepts with practical implications.
"""

        generation_time = time.time() - start_time
        logging.info(
            f"‚úÖ Improved single source prompt generation completed in {generation_time:.2f}s - prompt length: {len(prompt)} characters"
        )

        return prompt

    def create_improved_multi_source_prompt(
        self, data: Dict[str, Any], context: Dict[str, Any]
    ) -> str:
        """
        Create improved multi-source analysis prompt (4000+ words, narrative-focused).
        Focuses on correlation, PCA, and cross-source synthesis with practical interpretation.
        INTERPRETS ACTUAL PCA RESULTS - does not hardcode PC1/PC2 meanings.

        Args:
            data: Aggregated analysis data from multiple sources
            context: Additional context for analysis

        Returns:
            Multi-source analysis prompt string with narrative focus
        """
        import time

        start_time = time.time()
        logging.info(
            f"üìù Starting improved multi-source prompt generation for '{data.get('tool_name', 'Unknown')}' in {self.language}"
        )

        # Extract key information
        tool_name = data.get("tool_name", "Unknown Tool")
        sources = data.get("selected_sources", [])
        pca_insights = data.get("pca_insights", {})
        heatmap_data = data.get("heatmap_analysis", {})
        date_range = f"del {data.get('date_range_start', 'N/A')} al {data.get('date_range_end', 'N/A')}"
        data_points = data.get("data_points_analyzed", 0)

        # Extract actual PCA results for dynamic interpretation
        pca_components = pca_insights.get("dominant_patterns", [])
        variance_explained = pca_insights.get("total_variance_explained", 0)

        # Build the improved narrative prompt with data-driven PCA interpretation
        if self.language == "es":
            prompt = f"""
AN√ÅLISIS NARRATIVO MEJORADO MULTI-FUENTE - HERRAMIENTAS DE GESTI√ìN
Herramienta Analizada: {tool_name}
Fuentes de Datos: {", ".join(sources)}
Per√≠odo: {date_range}
Fecha del An√°lisis: {datetime.now().strftime("%Y-%m-%d")}

=== CONTEXTO DEL AN√ÅLISIS ===

**Enfoque Narrativo Empresarial Multi-Fuente:**
Este an√°lisis integra insights de m√∫ltiples fuentes de datos para proporcionar una perspectiva empresarial hol√≠stica. Se enfoca en la interpretaci√≥n estrat√©gica basada en LOS RESULTADOS REALES, no en an√°lisis predeterminado.

**Datos Disponibles (S√≠ntesis Interpretativa):**
- An√°lisis de correlaci√≥n entre fuentes m√∫ltiples
- An√°lisis de Componentes Principales (PCA) con cargas y componentes
- Mapa de calor y patrones visuales de correlaci√≥n
- An√°lisis temporal combinado de m√∫ltiples fuentes
- {data_points:,} puntos de datos integrados del per√≠odo {date_range}

**RESULTADOS PCA REALES PARA INTERPRETAR:**
- Varianza Explicada Total: {variance_explained:.1f}%
- N√∫mero de Componentes: {len(pca_components)}
- **IMPORTANTE**: Interprete estos componentes espec√≠ficos, no significado predeterminado

=== ESTRUCTURA REQUERIDA (4000+ PALABRAS) ===

**SECCI√ìN 1: RESUMEN EJECUTIVO** (400 palabras)
- Implicaciones estrat√©gicas de la perspectiva multi-fuente
- Patrones clave a trav√©s de m√∫ltiples fuentes de datos
- Insights de brecha teor√≠a-pr√°ctica
- Recomendaciones de adopci√≥n empresarial

**SECCI√ìN 2: AN√ÅLISIS DE CORRELACI√ìN MULTI-FUENTE** (800 palabras) [PRIMARIO]
- Interpretaci√≥n de relaciones entre fuentes de datos
- Fortalezas de correlaci√≥n y su significado empresarial
- Patrones de oposici√≥n y lo que revelan sobre adopci√≥n
- Se√±ales de mercado desde patrones de correlaci√≥n
- Validaci√≥n cruzada entre fuentes

**SECCI√ìN 3: AN√ÅLISIS DE COMPONENTES PRINCIPALES (PCA)** (1000 palabras) [PRIMARIO]
- **INTERPRETACI√ìN DATA-DRIVEN**: Use los componentes espec√≠ficos calculados
- Analice las cargas reales de cada fuente en cada componente
- Explique qu√© patrones reales revelan estos componentes
- Relaciones entre fuentes y patrones de oposici√≥n OBSERVADOS
- Varianza explicada real y lo que revela sobre complejidad
- **NO asuma significados predeterminados** - interprete los resultados reales

**SECCI√ìN 4: AN√ÅLISIS DE PERIODOGRAMA Y FOURIER COMBINADO** (800 palabras) [PRIMARIO]
- An√°lisis espectral combinado a trav√©s de todas las fuentes
- Ciclos dominantes y su significado empresarial
- Patrones de frecuencia indicando ondas de adopci√≥n
- Indicadores de madurez del mercado desde an√°lisis espectral
- Insights de timing estrat√©gico desde an√°lisis c√≠clico

**SECCI√ìN 5: S√çNTESIS TEMPORAL MULTI-FUENTE** (600 palabras)
- Tendencias a largo plazo a trav√©s de m√∫ltiples fuentes
- Interpretaci√≥n de ciclo de adopci√≥n
- Indicadores de madurez del mercado
- Implicaciones de trayectoria futura

**SECCI√ìN 6: INSIGHTS DE IMPLEMENTACI√ìN ESTRAT√âGICA** (400 palabras)
- Recomendaciones accionables basadas en an√°lisis multi-fuente
- Factores de riesgo y indicadores de √©xito
- Timing y enfoque de implementaci√≥n
- Implicaciones de ventaja competitiva

=== INSTRUCCIONES DE AN√ÅLISIS ===

**ENFOQUE DATA-DRIVEN ESPECIALMENTE PARA PCA:**
- Examine las cargas reales de cada fuente en cada componente
- Identifique qu√© fuentes tienen influencia alta vs baja en cada componente
- Observe tensiones reales (cargas opuestas) entre fuentes
- Interprete la varianza explicada real en t√©rminos de complejidad del mercado
- Conecte patrones observados con teor√≠a empresarial

**Enfoque Narrativo Sobre Estad√≠stico:**
- NO presente coeficientes de correlaci√≥n espec√≠ficos
- NO reporte varianza explicada num√©ricamente
- S√ç interprete: "Las fuentes muestran fuerte alineaci√≥n, sugiriendo..."
- S√ç conecte patrones con din√°mica de mercado
- S√ç proporcione insights estrat√©gicos accionables

**Conexiones Estrat√©gicas Multi-Fuente:**
1. Validar patrones mediante concordancia entre fuentes
2. Identificar tensiones mediante discordancia entre fuentes
3. Posicionar insights en contexto competitivo
4. Traducir hallazgos t√©cnicos en decisiones empresariales

**Rigor Acad√©mico-Profesional:**
- Mantenga est√°ndares acad√©micos pero accesible para ejecutivos
- Conecte teor√≠a de gesti√≥n con pr√°ctica empresarial
- Use terminolog√≠a profesional precisa
- Proporcione insights diferenciadores y accionables

**PROHIBICIONES ABSOLUTAS:**
- NO incluir secci√≥n de Referencias
- NO presentar matrices de correlaci√≥n num√©ricas
- NO usar formato de vi√±etas para el an√°lisis principal
- NO repetir estad√≠sticas del dashboard
- NO asignar significados predeterminados a componentes PCA

**RESULTADO ESPERADO:**
Un ensayo narrativo integrado de 4000+ palabras que interprete LOS RESULTADOS REALES de m√∫ltiples fuentes de datos en insights estrat√©gicos coherentes, con √©nfasis en correlaciones, PCA y patrones espectrales como fuentes primarias de insights empresariales.
"""
        else:
            prompt = f"""
IMPROVED MULTI-SOURCE NARRATIVE ANALYSIS - MANAGEMENT TOOLS
Tool Analyzed: {tool_name}
Data Sources: {", ".join(sources)}
Period: {date_range}
Analysis Date: {datetime.now().strftime("%Y-%m-%d")}

=== ANALYSIS CONTEXT ===

**Multi-Source Business Narrative Focus:**
This analysis integrates insights from multiple data sources to provide a holistic business perspective. Focuses on strategic interpretation based on ACTUAL RESULTS, not predetermined analysis.

**Available Data (Interpretive Synthesis):**
- Correlation analysis between multiple sources
- Principal Component Analysis (PCA) with loadings and components
- Heatmap and visual correlation patterns
- Combined temporal analysis from multiple sources
- {data_points:,} integrated data points from period {date_range}

**ACTUAL PCA RESULTS TO INTERPRET:**
- Total Explained Variance: {variance_explained:.1f}%
- Number of Components: {len(pca_components)}
- **IMPORTANT**: Interpret these specific components, not predetermined meaning

=== REQUIRED STRUCTURE (4000+ WORDS) ===

**SECTION 1: EXECUTIVE OVERVIEW** (400 words)
- Strategic implications from multi-source perspective
- Key patterns across multiple data sources
- Theory-practice gap insights
- Business adoption recommendations

**SECTION 2: MULTI-SOURCE CORRELATION ANALYSIS** (800 words) [PRIMARY]
- Relationship interpretation between multiple data sources
- Correlation strengths and their business meaning
- Opposition patterns and what they reveal about tool adoption
- Market signals from correlation patterns
- Cross-source validation

**SECTION 3: PRINCIPAL COMPONENT ANALYSIS (PCA)** (1000 words) [PRIMARY]
- **DATA-DRIVEN INTERPRETATION**: Use the specific calculated components
- Analyze real loadings of each source on each component
- Explain what patterns these specific components reveal
- Observed source relationships and opposition patterns
- Real explained variance and what it reveals about complexity
- **DO NOT assume predetermined meanings** - interpret actual results

**SECTION 4: COMBINED PERIODOGRAM AND FOURIER ANALYSIS** (800 words) [PRIMARY]
- Combined spectral analysis across all sources
- Dominant cycles and their business significance
- Frequency patterns indicating adoption waves
- Market maturity indicators from spectral analysis
- Strategic timing insights from cyclical analysis

**SECTION 5: MULTI-SOURCE TEMPORAL SYNTHESIS** (600 words)
- Long-term trends across multiple sources
- Adoption lifecycle interpretation
- Market maturity indicators
- Future trajectory implications

**SECTION 6: STRATEGIC IMPLEMENTATION INSIGHTS** (400 words)
- Actionable recommendations based on multi-source analysis
- Risk factors and success indicators
- Implementation timing and approach
- Competitive advantage implications

=== ANALYSIS INSTRUCTIONS ===

**DATA-DRIVEN APPROACH ESPECIALLY FOR PCA:**
- Examine real loadings of each source on each component
- Identify which sources have high vs low influence on each component
- Observe real tensions (opposite loadings) between sources
- Interpret real explained variance in terms of market complexity
- Connect observed patterns with business theory

**Narrative Over Statistical Focus:**
- DO NOT present specific correlation coefficients
- DO NOT report numerical variance explained
- DO interpret: "Sources show strong alignment, suggesting..."
- DO connect patterns with market dynamics
- DO provide actionable strategic insights

**Multi-Source Strategic Connections:**
1. Validate patterns through source agreement
2. Identify tensions through source discordance
3. Position insights in competitive context
4. Translate technical findings into business decisions

**Academic-Professional Rigor:**
- Maintain academic standards but accessible to executives
- Connect management theory with business practice
- Use precise professional terminology
- Provide differentiating and actionable insights

**ABSOLUTE PROHIBITIONS:**
- DO NOT include References section
- DO NOT present numerical correlation matrices
- DO NOT use bullet format for main analysis
- DO NOT repeat dashboard statistics
- DO NOT assign predetermined meanings to PCA components

**EXPECTED RESULT:**
A integrated narrative essay of 4000+ words that interprets ACTUAL RESULTS from multiple data sources into coherent strategic insights, with emphasis on correlations, PCA, and spectral patterns as primary sources of business insights.
"""

        generation_time = time.time() - start_time
        logging.info(
            f"‚úÖ Improved multi-source prompt generation completed in {generation_time:.2f}s - prompt length: {len(prompt)} characters"
        )

        return prompt

    def _load_templates(self) -> Dict[str, Dict[str, str]]:
        """Load bilingual prompt templates."""
        return {
            "comprehensive_analysis": {
                "es": """
AN√ÅLISIS DOCTORAL DE HERRAMIENTAS DE GESTI√ìN
Fecha: {analysis_date}

{context}

Por favor, genera un an√°lisis doctoral-level que integre todos los elementos anteriores.
""",
                "en": """
DOCTORAL-LEVEL MANAGEMENT TOOLS ANALYSIS
Date: {analysis_date}

{context}

Please generate a doctoral-level analysis that integrates all the above elements.
""",
            },
            "pca_focused": {
                "es": """
AN√ÅLISIS ENFOCADO EN PCA DE HERRAMIENTAS DE GESTI√ìN
Fecha: {analysis_date}

{pca_analysis}

Genera insights profundos basados en el an√°lisis de componentes principales.
""",
                "en": """
PCA-FOCUSED MANAGEMENT TOOLS ANALYSIS
Date: {analysis_date}

{pca_analysis}

Generate deep insights based on principal component analysis.
""",
            },
            "executive_summary": {
                "es": """
RESUMEN EJECUTIVO DE HERRAMIENTAS DE GESTI√ìN
Fecha: {executive_date}

{executive_content}

Genera un resumen conciso y accionable para l√≠deres empresariales.
""",
                "en": """
EXECUTIVE SUMMARY OF MANAGEMENT TOOLS
Date: {executive_date}

{executive_content}

Generate a concise, actionable summary for business leaders.
""",
            },
            "single_source_analysis": {
                "es": """
AN√ÅLISIS DE FUENTE √öNICA DE HERRAMIENTAS DE GESTI√ìN
Fecha: {analysis_date}

{context}

Por favor, genera un an√°lisis doctoral-level que integre todos los elementos anteriores.
""",
                "en": """
SINGLE SOURCE MANAGEMENT TOOLS ANALYSIS
Date: {analysis_date}

{context}

Please generate a doctoral-level analysis that integrates all the above elements.
""",
            },
        }
